{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GOT Project.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trong-shen/Game-of-Throne-Project/blob/master/GOT_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kvbsdq0-guNm",
        "colab_type": "text"
      },
      "source": [
        "Load the CSV file from Github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt3EaJ_AgV2I",
        "colab_type": "code",
        "outputId": "3413ff56-35cc-4664-ed77-e45b0cd4a6f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "\n",
        "\n",
        "GOT= pd.read_csv('https://raw.githubusercontent.com/trong-shen/Game-of-Throne-Project/master/Game_of_Thrones_Script_clean.csv')\n",
        "char_info=pd.read_csv('https://raw.githubusercontent.com/trong-shen/Game-of-Throne-Project/master/got_table.csv')\n",
        "print(len(GOT))\n",
        "\n",
        "#Extract only the house data from char_info\n",
        "House=char_info[['name','house']]\n",
        "\n",
        "#Created a function to apply globally to the data frame\n",
        "def return_house(name):\n",
        "  house_dict=dict(zip(House.name,House.house))\n",
        "  try: \n",
        "    house=house_dict[name]\n",
        "    return(house)\n",
        "  except KeyError:\n",
        "    return(float(\"Nan\"))\n",
        "\n",
        "# Apply the house dict function to the whole GOT dataframe\n",
        "GOT['House']=GOT['Name'].apply(lambda x:return_house(x))\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "23911\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9thUHG5FPCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT79c3JB_d23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#list of contractions and the expanded mapping used for cleaning the data\n",
        "CONTRACTION_MAP = {\n",
        "\"ain't\": \"is not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"I'd\": \"I would\",\n",
        "\"I'd've\": \"I would have\",\n",
        "\"I'll\": \"I will\",\n",
        "\"I'll've\": \"I will have\",\n",
        "\"I'm\": \"I am\",\n",
        "\"I've\": \"I have\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cle0cONgAULs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define a function, expand_contractions, which takes a string and expands all contractions within the string using contraction_map\n",
        "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
        "    expanded = ''\n",
        "    text = text.lower() #make all text lowercase\n",
        "    wordList = text.split() #put text into a list of words\n",
        "    for i in range(len(wordList)):\n",
        "        if wordList[i] in contraction_mapping.keys(): #for each word, if it is a contraction in the listing\n",
        "            expanded = expanded + ' ' + contraction_mapping[wordList[i]] #then replace with the expanded version\n",
        "        else:\n",
        "            expanded = expanded + ' ' + wordList[i] #otherwise, keep the original word\n",
        "    return expanded\n",
        "\n",
        "#define a function, remove_punctuation, which takes in a string and removes all punctuation \n",
        "def remove_punctuation(s):\n",
        "    s = s.translate(str.maketrans('','',string.punctuation)) #take out punctuation in the sentence\n",
        "    j = nltk.word_tokenize(s.lower()) #put each word in the sentence within a list, j\n",
        "    return s\n",
        "\n",
        "#define function, clean_sentences, which removes punctuation and expands all contractions in a sentence\n",
        "def clean_sentences(text):\n",
        "    return remove_punctuation(expand_contractions(text))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOXNk319AXLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Expand contractions, remove punctuation all in one function clean_sentence\n",
        "GOT['Sentences_Clean'] = GOT.Sentence.apply(lambda x:clean_sentences(x))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS53Yh89CAnb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calculate words per line, assuming contractions are all expanded\n",
        "GOT[\"Num_Words\"] = GOT.Sentences_Clean.apply(lambda x: len(x.split()))\n",
        "GOT.to_csv (r'GOT_house_csv.csv', index = False, header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5ln7QIcDK6q",
        "colab_type": "text"
      },
      "source": [
        "# Now we will  prepare data for our Machine Learning Predictive model by only looking at msotly season 1 data and a subsection of season 2 data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdrjeoBTukgp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "0e53f1df-9508-4385-e70a-46eac9dd482b"
      },
      "source": [
        "#Extract only season one data and two data\n",
        "GOT1=GOT[GOT.Season==\"Season 1\"]\n",
        "GOT2=GOT[GOT.Season==\"Season 2\"]\n",
        "\n",
        "print(GOT1.head())\n",
        "print(GOT1.info())\n",
        "\n",
        "print(GOT2.head())\n",
        "print(GOT1.info())"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Release Date  ... Num_Words\n",
            "0    4/17/2011  ...        27\n",
            "1    4/17/2011  ...        23\n",
            "2    4/17/2011  ...         5\n",
            "3    4/17/2011  ...         5\n",
            "4    4/17/2011  ...         7\n",
            "\n",
            "[5 rows x 9 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 3179 entries, 0 to 3178\n",
            "Data columns (total 9 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   Release Date     3179 non-null   object\n",
            " 1   Season           3179 non-null   object\n",
            " 2   Episode          3179 non-null   object\n",
            " 3   Episode Title    3179 non-null   object\n",
            " 4   Name             3179 non-null   object\n",
            " 5   Sentence         3179 non-null   object\n",
            " 6   House            2907 non-null   object\n",
            " 7   Sentences_Clean  3179 non-null   object\n",
            " 8   Num_Words        3179 non-null   int64 \n",
            "dtypes: int64(1), object(8)\n",
            "memory usage: 248.4+ KB\n",
            "None\n",
            "     Release Date    Season  ...                     Sentences_Clean Num_Words\n",
            "3179     4/1/2012  Season 2  ...        well struckâ€¦ well struck dog         5\n",
            "3180     4/1/2012  Season 2  ...                   did you like that         4\n",
            "3181     4/1/2012  Season 2  ...       it was well struck your grace         6\n",
            "3182     4/1/2012  Season 2  ...   i already said it was well struck         7\n",
            "3183     4/1/2012  Season 2  ...                      yes your grace         3\n",
            "\n",
            "[5 rows x 9 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 3179 entries, 0 to 3178\n",
            "Data columns (total 9 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   Release Date     3179 non-null   object\n",
            " 1   Season           3179 non-null   object\n",
            " 2   Episode          3179 non-null   object\n",
            " 3   Episode Title    3179 non-null   object\n",
            " 4   Name             3179 non-null   object\n",
            " 5   Sentence         3179 non-null   object\n",
            " 6   House            2907 non-null   object\n",
            " 7   Sentences_Clean  3179 non-null   object\n",
            " 8   Num_Words        3179 non-null   int64 \n",
            "dtypes: int64(1), object(8)\n",
            "memory usage: 248.4+ KB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbaevQcMf3Qj",
        "colab_type": "code",
        "outputId": "6d518d2d-d980-48e5-9ccb-48c855e08d50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Keep character lines if and only if they still exist in season 2\n",
        "\n",
        "#Find a unique list of characters in season 2\n",
        "\n",
        "char_S2=GOT2.Name.unique()\n",
        "print(len(char_S2))\n",
        "\n",
        "#Filter S1 data if characters are in S2\n",
        "GOT1_modified=GOT1[GOT1['Name'].isin(char_S2)]\n",
        "print(len(GOT1_modified.Name.unique()))\n",
        "print(GOT1_modified.Name.unique())"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "136\n",
            "49\n",
            "['jon snow' 'sansa stark' 'robb stark' 'catelyn stark' 'bran stark'\n",
            " 'theon greyjoy' 'jaime lannister' 'cersei lannister' 'luwin' 'arya stark'\n",
            " 'tyrion lannister' 'ros' 'daenerys targaryen' 'jorah mormont'\n",
            " 'khal drogo' 'sandor clegane' 'doreah' 'irri' 'joffrey lannister'\n",
            " 'myrcella baratheon' 'rodrick cassel' 'soldier' 'varys' 'renly baratheon'\n",
            " 'petyr baelish' 'grand maester pycelle' 'guard' 'jeor mormont' 'grenn'\n",
            " 'lancel lannister' 'rakharo' 'yoren' 'sam tarly' 'janos'\n",
            " 'gendry baratheon' 'bronn' 'loras tyrell' 'osha' 'wildling' 'man'\n",
            " 'tywin lannister' 'meryn trant' 'kevan lannister' 'all' 'prostitute'\n",
            " 'shae' 'rickon stark' 'karstark' 'hot pie']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUhNemrdcXNg",
        "colab_type": "code",
        "outputId": "daa6d76b-9471-4868-eea5-df0fd8ed9888",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "#Further filter based on characters of interest \n",
        "#removing generic characters such as solider, guard, prostitute, etc\n",
        "important_names=np.delete(GOT1_modified.Name.unique(),(21,26,38,39,43,44));\n",
        "print(important_names)\n",
        "\n",
        "\n",
        "print(len(important_names))\n",
        "\n",
        "GOT1_modified=GOT1_modified[GOT1_modified['Name'].isin(important_names)]\n",
        "GOT1_modified.head()\n",
        "print(len(GOT1_modified.Name.unique()))\n",
        "\n",
        "GOT1_modified.info()\n",
        "GOT1=GOT1_modified\n",
        "GOT1.info()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['jon snow' 'sansa stark' 'robb stark' 'catelyn stark' 'bran stark'\n",
            " 'theon greyjoy' 'jaime lannister' 'cersei lannister' 'luwin' 'arya stark'\n",
            " 'tyrion lannister' 'ros' 'daenerys targaryen' 'jorah mormont'\n",
            " 'khal drogo' 'sandor clegane' 'doreah' 'irri' 'joffrey lannister'\n",
            " 'myrcella baratheon' 'rodrick cassel' 'petyr baelish'\n",
            " 'grand maester pycelle' 'jeor mormont' 'lancel lannister' 'yoren'\n",
            " 'sam tarly' 'janos' 'gendry baratheon' 'bronn' 'loras tyrell' 'osha'\n",
            " 'tywin lannister' 'meryn trant' 'rickon stark' 'karstark']\n",
            "36\n",
            "36\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 1907 entries, 15 to 3178\n",
            "Data columns (total 10 columns):\n",
            " #   Column              Non-Null Count  Dtype \n",
            "---  ------              --------------  ----- \n",
            " 0   Release Date        1907 non-null   object\n",
            " 1   Season              1907 non-null   object\n",
            " 2   Episode             1907 non-null   object\n",
            " 3   Episode Title       1907 non-null   object\n",
            " 4   Name                1907 non-null   object\n",
            " 5   Sentence            1907 non-null   object\n",
            " 6   House               1868 non-null   object\n",
            " 7   Sentences_Clean     1907 non-null   object\n",
            " 8   Num_Words           1907 non-null   int64 \n",
            " 9   Tokenized_Sentence  1907 non-null   object\n",
            "dtypes: int64(1), object(9)\n",
            "memory usage: 163.9+ KB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 1907 entries, 15 to 3178\n",
            "Data columns (total 10 columns):\n",
            " #   Column              Non-Null Count  Dtype \n",
            "---  ------              --------------  ----- \n",
            " 0   Release Date        1907 non-null   object\n",
            " 1   Season              1907 non-null   object\n",
            " 2   Episode             1907 non-null   object\n",
            " 3   Episode Title       1907 non-null   object\n",
            " 4   Name                1907 non-null   object\n",
            " 5   Sentence            1907 non-null   object\n",
            " 6   House               1868 non-null   object\n",
            " 7   Sentences_Clean     1907 non-null   object\n",
            " 8   Num_Words           1907 non-null   int64 \n",
            " 9   Tokenized_Sentence  1907 non-null   object\n",
            "dtypes: int64(1), object(9)\n",
            "memory usage: 163.9+ KB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOo0mZPmOaqr",
        "colab_type": "code",
        "outputId": "d1abbfd3-1f8c-411e-c108-aafb3fe9d455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Tokenize the words and remove between words punctunations\n",
        "tokenizer=nltk.RegexpTokenizer(r\"\\w+\")\n",
        "GOT1['Tokenized_Sentence']=GOT1.Sentences_Clean.apply(lambda x:tokenizer.tokenize(x.lower()))\n",
        "GOT1.head(100)\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Release Date</th>\n",
              "      <th>Season</th>\n",
              "      <th>Episode</th>\n",
              "      <th>Episode Title</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>House</th>\n",
              "      <th>Sentences_Clean</th>\n",
              "      <th>Num_Words</th>\n",
              "      <th>Tokenized_Sentence</th>\n",
              "      <th>Tokenized_No_Stop</th>\n",
              "      <th>Stemmed_Sentence</th>\n",
              "      <th>Stemmed_Sentence_Non_Token</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>Go on. Father's watching.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>go on fathers watching</td>\n",
              "      <td>4</td>\n",
              "      <td>[go, on, fathers, watching]</td>\n",
              "      <td>[go, father, watching]</td>\n",
              "      <td>[go, father, watching]</td>\n",
              "      <td>go father watching</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>And your mother.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>and your mother</td>\n",
              "      <td>3</td>\n",
              "      <td>[and, your, mother]</td>\n",
              "      <td>[mother]</td>\n",
              "      <td>[mother]</td>\n",
              "      <td>mother</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>sansa stark</td>\n",
              "      <td>Thank you.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>thank you</td>\n",
              "      <td>2</td>\n",
              "      <td>[thank, you]</td>\n",
              "      <td>[thank]</td>\n",
              "      <td>[thank]</td>\n",
              "      <td>thank</td>\n",
              "      <td>{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>Don't think too much, Bran.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>do not think too much bran</td>\n",
              "      <td>6</td>\n",
              "      <td>[do, not, think, too, much, bran]</td>\n",
              "      <td>[think, much, bran]</td>\n",
              "      <td>[think, much, bran]</td>\n",
              "      <td>think much bran</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>robb stark</td>\n",
              "      <td>Relax your bow arm.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>relax your bow arm</td>\n",
              "      <td>4</td>\n",
              "      <td>[relax, your, bow, arm]</td>\n",
              "      <td>[relax, bow, arm]</td>\n",
              "      <td>[relax, bow, arm]</td>\n",
              "      <td>relax bow arm</td>\n",
              "      <td>{'neg': 0.0, 'neu': 0.408, 'pos': 0.592, 'comp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>tyrion lannister</td>\n",
              "      <td>She has odd cravings, our sister.</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>she has odd cravings our sister</td>\n",
              "      <td>6</td>\n",
              "      <td>[she, has, odd, cravings, our, sister]</td>\n",
              "      <td>[odd, cravings, sister]</td>\n",
              "      <td>[odd, craving, sister]</td>\n",
              "      <td>odd craving sister</td>\n",
              "      <td>{'neg': 0.535, 'neu': 0.465, 'pos': 0.0, 'comp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jaime lannister</td>\n",
              "      <td>A family trait. Now, the Starks are feasting u...</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>a family trait now the starks are feasting us...</td>\n",
              "      <td>19</td>\n",
              "      <td>[a, family, trait, now, the, starks, are, feas...</td>\n",
              "      <td>[family, trait, starks, feasting, us, sundown,...</td>\n",
              "      <td>[family, trait, starks, feasting, u, sundown, ...</td>\n",
              "      <td>family trait starks feasting u sundown leave ...</td>\n",
              "      <td>{'neg': 0.348, 'neu': 0.652, 'pos': 0.0, 'comp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>tyrion lannister</td>\n",
              "      <td>I'm sorry, I've begun the feast a bit early. A...</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>i am sorry i have begun the feast a bit early...</td>\n",
              "      <td>19</td>\n",
              "      <td>[i, am, sorry, i, have, begun, the, feast, a, ...</td>\n",
              "      <td>[sorry, begun, feast, bit, early, first, many,...</td>\n",
              "      <td>[sorry, begun, feast, bit, early, first, many,...</td>\n",
              "      <td>sorry begun feast bit early first many course</td>\n",
              "      <td>{'neg': 0.157, 'neu': 0.843, 'pos': 0.0, 'comp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jaime lannister</td>\n",
              "      <td>I thought you might say that. But since we're ...</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>i thought you might say that but since we are...</td>\n",
              "      <td>20</td>\n",
              "      <td>[i, thought, you, might, say, that, but, since...</td>\n",
              "      <td>[thought, might, say, since, short, time, come...</td>\n",
              "      <td>[thought, might, say, since, short, time, come...</td>\n",
              "      <td>thought might say since short time come girl ...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>tyrion lannister</td>\n",
              "      <td>Close the door!</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>close the door</td>\n",
              "      <td>3</td>\n",
              "      <td>[close, the, door]</td>\n",
              "      <td>[close, door]</td>\n",
              "      <td>[close, door]</td>\n",
              "      <td>close door</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows Ã— 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Release Date  ...                                          Sentiment\n",
              "15     4/17/2011  ...  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...\n",
              "16     4/17/2011  ...  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...\n",
              "18     4/17/2011  ...  {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound...\n",
              "21     4/17/2011  ...  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...\n",
              "22     4/17/2011  ...  {'neg': 0.0, 'neu': 0.408, 'pos': 0.592, 'comp...\n",
              "..           ...  ...                                                ...\n",
              "163    4/17/2011  ...  {'neg': 0.535, 'neu': 0.465, 'pos': 0.0, 'comp...\n",
              "164    4/17/2011  ...  {'neg': 0.348, 'neu': 0.652, 'pos': 0.0, 'comp...\n",
              "165    4/17/2011  ...  {'neg': 0.157, 'neu': 0.843, 'pos': 0.0, 'comp...\n",
              "166    4/17/2011  ...  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...\n",
              "167    4/17/2011  ...  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...\n",
              "\n",
              "[100 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyBF3RVqPFf1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Remove stopwords for sentiment analysis\n",
        "stopword=nltk.corpus.stopwords.words('english')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J8WRWnGeFvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define a function to remove stop words\n",
        "def remove_stopwords(tokenized_sentence):\n",
        "  text=[word for word in tokenized_sentence if word not in stopword]\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGBoLfGGefi_",
        "colab_type": "code",
        "outputId": "0ef45f34-6077-4a80-9a8e-67fe8c4ef574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "#Implement the stop words function to a new column \n",
        "GOT1['Tokenized_No_Stop']=GOT1.Tokenized_Sentence.apply(lambda x:remove_stopwords(x))\n",
        "GOT1.head()"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Release Date</th>\n",
              "      <th>Season</th>\n",
              "      <th>Episode</th>\n",
              "      <th>Episode Title</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>House</th>\n",
              "      <th>Sentences_Clean</th>\n",
              "      <th>Num_Words</th>\n",
              "      <th>Tokenized_Sentence</th>\n",
              "      <th>Tokenized_No_Stop</th>\n",
              "      <th>Stemmed_Sentence</th>\n",
              "      <th>Stemmed_Sentence_Non_Token</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>Go on. Father's watching.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>go on fathers watching</td>\n",
              "      <td>4</td>\n",
              "      <td>[go, on, fathers, watching]</td>\n",
              "      <td>[go, fathers, watching]</td>\n",
              "      <td>[go, father, watching]</td>\n",
              "      <td>go father watching</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>And your mother.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>and your mother</td>\n",
              "      <td>3</td>\n",
              "      <td>[and, your, mother]</td>\n",
              "      <td>[mother]</td>\n",
              "      <td>[mother]</td>\n",
              "      <td>mother</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>sansa stark</td>\n",
              "      <td>Thank you.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>thank you</td>\n",
              "      <td>2</td>\n",
              "      <td>[thank, you]</td>\n",
              "      <td>[thank]</td>\n",
              "      <td>[thank]</td>\n",
              "      <td>thank</td>\n",
              "      <td>{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>Don't think too much, Bran.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>do not think too much bran</td>\n",
              "      <td>6</td>\n",
              "      <td>[do, not, think, too, much, bran]</td>\n",
              "      <td>[think, much, bran]</td>\n",
              "      <td>[think, much, bran]</td>\n",
              "      <td>think much bran</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>robb stark</td>\n",
              "      <td>Relax your bow arm.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>relax your bow arm</td>\n",
              "      <td>4</td>\n",
              "      <td>[relax, your, bow, arm]</td>\n",
              "      <td>[relax, bow, arm]</td>\n",
              "      <td>[relax, bow, arm]</td>\n",
              "      <td>relax bow arm</td>\n",
              "      <td>{'neg': 0.0, 'neu': 0.408, 'pos': 0.592, 'comp...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Release Date  ...                                          Sentiment\n",
              "15    4/17/2011  ...  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...\n",
              "16    4/17/2011  ...  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...\n",
              "18    4/17/2011  ...  {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound...\n",
              "21    4/17/2011  ...  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...\n",
              "22    4/17/2011  ...  {'neg': 0.0, 'neu': 0.408, 'pos': 0.592, 'comp...\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnF0bKWXe63D",
        "colab_type": "code",
        "outputId": "fa983e7b-76e4-4628-a0ef-5de4c5c38090",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "# Stemming of the Non Stop Words Column\n",
        "wn=nltk.WordNetLemmatizer()\n",
        "\n",
        "def stem_reduction(tokenized_sentence):\n",
        "  sentence=[wn.lemmatize(word) for word in tokenized_sentence]\n",
        "  return (sentence)\n",
        "\n",
        "GOT1['Stemmed_Sentence']=GOT1['Tokenized_No_Stop'].apply(lambda x:stem_reduction(x))\n",
        "GOT1.tail()"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Release Date</th>\n",
              "      <th>Season</th>\n",
              "      <th>Episode</th>\n",
              "      <th>Episode Title</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>House</th>\n",
              "      <th>Sentences_Clean</th>\n",
              "      <th>Num_Words</th>\n",
              "      <th>Tokenized_Sentence</th>\n",
              "      <th>Tokenized_No_Stop</th>\n",
              "      <th>Stemmed_Sentence</th>\n",
              "      <th>Stemmed_Sentence_Non_Token</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3173</th>\n",
              "      <td>6/19/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 10</td>\n",
              "      <td>Fire and Blood</td>\n",
              "      <td>jorah mormont</td>\n",
              "      <td>Don't ask me to stand aside as you climb on th...</td>\n",
              "      <td>House Mormont</td>\n",
              "      <td>do not ask me to stand aside as you climb on ...</td>\n",
              "      <td>19</td>\n",
              "      <td>[do, not, ask, me, to, stand, aside, as, you, ...</td>\n",
              "      <td>[ask, stand, aside, climb, pyre, l, watch, burn]</td>\n",
              "      <td>[ask, stand, aside, climb, pyre, l, watch, burn]</td>\n",
              "      <td>ask stand aside climb pyre l watch burn</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3174</th>\n",
              "      <td>6/19/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 10</td>\n",
              "      <td>Fire and Blood</td>\n",
              "      <td>daenerys targaryen</td>\n",
              "      <td>ls that what you fear? You will be my khalasar...</td>\n",
              "      <td>House Targaryen</td>\n",
              "      <td>ls that what you fear you will be my khalasar...</td>\n",
              "      <td>47</td>\n",
              "      <td>[ls, that, what, you, fear, you, will, be, my,...</td>\n",
              "      <td>[ls, fear, khalasar, l, see, faces, slaves, l,...</td>\n",
              "      <td>[l, fear, khalasar, l, see, face, slave, l, fr...</td>\n",
              "      <td>l fear khalasar l see face slave l free take ...</td>\n",
              "      <td>{'neg': 0.221, 'neu': 0.533, 'pos': 0.246, 'co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3175</th>\n",
              "      <td>6/19/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 10</td>\n",
              "      <td>Fire and Blood</td>\n",
              "      <td>daenerys targaryen</td>\n",
              "      <td>Ser Jorah, bind this woman to the pyre. You sw...</td>\n",
              "      <td>House Targaryen</td>\n",
              "      <td>ser jorah bind this woman to the pyre you swo...</td>\n",
              "      <td>13</td>\n",
              "      <td>[ser, jorah, bind, this, woman, to, the, pyre,...</td>\n",
              "      <td>[ser, jorah, bind, woman, pyre, swore, obey]</td>\n",
              "      <td>[ser, jorah, bind, woman, pyre, swore, obey]</td>\n",
              "      <td>ser jorah bind woman pyre swore obey</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3177</th>\n",
              "      <td>6/19/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 10</td>\n",
              "      <td>Fire and Blood</td>\n",
              "      <td>daenerys targaryen</td>\n",
              "      <td>I will. But it is not your screams I want. Onl...</td>\n",
              "      <td>House Targaryen</td>\n",
              "      <td>i will but it is not your screams i want only...</td>\n",
              "      <td>13</td>\n",
              "      <td>[i, will, but, it, is, not, your, screams, i, ...</td>\n",
              "      <td>[screams, want, life]</td>\n",
              "      <td>[scream, want, life]</td>\n",
              "      <td>scream want life</td>\n",
              "      <td>{'neg': 0.54, 'neu': 0.2, 'pos': 0.26, 'compou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3178</th>\n",
              "      <td>6/19/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 10</td>\n",
              "      <td>Fire and Blood</td>\n",
              "      <td>jorah mormont</td>\n",
              "      <td>Blood of my blood.</td>\n",
              "      <td>House Mormont</td>\n",
              "      <td>blood of my blood</td>\n",
              "      <td>4</td>\n",
              "      <td>[blood, of, my, blood]</td>\n",
              "      <td>[blood, blood]</td>\n",
              "      <td>[blood, blood]</td>\n",
              "      <td>blood blood</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Release Date  ...                                          Sentiment\n",
              "3173    6/19/2011  ...  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...\n",
              "3174    6/19/2011  ...  {'neg': 0.221, 'neu': 0.533, 'pos': 0.246, 'co...\n",
              "3175    6/19/2011  ...  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...\n",
              "3177    6/19/2011  ...  {'neg': 0.54, 'neu': 0.2, 'pos': 0.26, 'compou...\n",
              "3178    6/19/2011  ...  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzKLVQ4f9bRK",
        "colab_type": "code",
        "outputId": "4c117698-af93-4a6f-fe2d-38df05b5f3d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#Join the Tokenized Stemmed_Sentence into a string of sentence for the VADER sentiment analysis\n",
        "\n",
        "def convert_sentence (tokenized_sentence):\n",
        "  sentence=''\n",
        "  for word in tokenized_sentence:\n",
        "    sentence=sentence+\" \"+word\n",
        "  return sentence\n",
        "\n",
        "GOT1['Stemmed_Sentence_Non_Token']=GOT1['Stemmed_Sentence'].apply(lambda x:convert_sentence(x))\n",
        "\n",
        "GOT1['Stemmed_Sentence_Non_Token'].head()\n",
        "\n",
        "\n"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15     go father watching\n",
              "16                 mother\n",
              "18                  thank\n",
              "21        think much bran\n",
              "22          relax bow arm\n",
              "Name: Stemmed_Sentence_Non_Token, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek4IbNjs6ob5",
        "colab_type": "code",
        "outputId": "999af842-cf09-4b84-aa19-5b5b00a696dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        }
      },
      "source": [
        "#Use Vader Sentiment analysis tool \n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sid=SentimentIntensityAnalyzer()\n",
        "\n",
        "GOT1['Sentiment']=GOT1['Stemmed_Sentence_Non_Token'].apply(lambda x:sid.polarity_scores(x))\n",
        "\n",
        "print(GOT1.Sentence[150])\n",
        "print(GOT1.Stemmed_Sentence[150])\n",
        "print(GOT1.Sentiment[150])\n",
        "GOT.head(100)\n"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The queen has two brothers?\n",
            "['queen', 'two', 'brother']\n",
            "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Release Date</th>\n",
              "      <th>Season</th>\n",
              "      <th>Episode</th>\n",
              "      <th>Episode Title</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>House</th>\n",
              "      <th>Sentences_Clean</th>\n",
              "      <th>Num_Words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>waymar royce</td>\n",
              "      <td>What do you expect? They're savages. One lot s...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>what do you expect they are savages one lot s...</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>will</td>\n",
              "      <td>I've never seen wildlings do a thing like this...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>i have never seen wildlings do a thing like t...</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>waymar royce</td>\n",
              "      <td>How close did you get?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>how close did you get</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>will</td>\n",
              "      <td>Close as any man would.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>close as any man would</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>gared</td>\n",
              "      <td>We should head back to the wall.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>we should head back to the wall</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>catelyn stark</td>\n",
              "      <td>You can always say no, Ned.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>you can always say no ned</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>catelyn stark</td>\n",
              "      <td>We need plenty of candles for Lord Tyrion's ch...</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>we need plenty of candles for lord tyrions ch...</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>maester luwin</td>\n",
              "      <td>I'm told he drinks all night.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>i am told he drinks all night</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>catelyn stark</td>\n",
              "      <td>How much could he possibly drink? A man of his...</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>how much could he possibly drink a man of his...</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>luwin</td>\n",
              "      <td>We've brought up eight barrels of ale from the...</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>we have brought up eight barrels of ale from ...</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows Ã— 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Release Date  ... Num_Words\n",
              "0     4/17/2011  ...        27\n",
              "1     4/17/2011  ...        23\n",
              "2     4/17/2011  ...         5\n",
              "3     4/17/2011  ...         5\n",
              "4     4/17/2011  ...         7\n",
              "..          ...  ...       ...\n",
              "95    4/17/2011  ...         6\n",
              "96    4/17/2011  ...        16\n",
              "97    4/17/2011  ...         7\n",
              "98    4/17/2011  ...        12\n",
              "99    4/17/2011  ...        16\n",
              "\n",
              "[100 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    }
  ]
}