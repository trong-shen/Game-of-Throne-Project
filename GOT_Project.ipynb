{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GOT Project.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trong-shen/Game-of-Throne-Project/blob/master/GOT_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kvbsdq0-guNm",
        "colab_type": "text"
      },
      "source": [
        "Load the CSV file from Github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt3EaJ_AgV2I",
        "colab_type": "code",
        "outputId": "fc22f912-c145-48ca-aaf1-a792dda80810",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "\n",
        "\n",
        "GOT= pd.read_csv('https://raw.githubusercontent.com/trong-shen/Game-of-Throne-Project/master/Game_of_Thrones_Script_clean.csv')\n",
        "char_info=pd.read_csv('https://raw.githubusercontent.com/trong-shen/Game-of-Throne-Project/master/got_table.csv')\n",
        "print(len(GOT))\n",
        "\n",
        "#Extract only the house data from char_info\n",
        "House=char_info[['name','house']]\n",
        "\n",
        "#Created a function to apply globally to the data frame\n",
        "def return_house(name):\n",
        "  house_dict=dict(zip(House.name,House.house))\n",
        "  try: \n",
        "    house=house_dict[name]\n",
        "    return(house)\n",
        "  except KeyError:\n",
        "    return(float(\"Nan\"))\n",
        "\n",
        "# Apply the house dict function to the whole GOT dataframe\n",
        "GOT['House']=GOT['Name'].apply(lambda x:return_house(x))\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "23911\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9thUHG5FPCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT79c3JB_d23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#list of contractions and the expanded mapping used for cleaning the data\n",
        "CONTRACTION_MAP = {\n",
        "\"ain't\": \"is not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"I'd\": \"I would\",\n",
        "\"I'd've\": \"I would have\",\n",
        "\"I'll\": \"I will\",\n",
        "\"I'll've\": \"I will have\",\n",
        "\"I'm\": \"I am\",\n",
        "\"I've\": \"I have\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cle0cONgAULs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define a function, expand_contractions, which takes a string and expands all contractions within the string using contraction_map\n",
        "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
        "    expanded = ''\n",
        "    text = text.lower() #make all text lowercase\n",
        "    wordList = text.split() #put text into a list of words\n",
        "    for i in range(len(wordList)):\n",
        "        if wordList[i] in contraction_mapping.keys(): #for each word, if it is a contraction in the listing\n",
        "            expanded = expanded + ' ' + contraction_mapping[wordList[i]] #then replace with the expanded version\n",
        "        else:\n",
        "            expanded = expanded + ' ' + wordList[i] #otherwise, keep the original word\n",
        "    return expanded\n",
        "\n",
        "#define a function, remove_punctuation, which takes in a string and removes all punctuation \n",
        "def remove_punctuation(s):\n",
        "    s = s.translate(str.maketrans('','',string.punctuation)) #take out punctuation in the sentence\n",
        "    j = nltk.word_tokenize(s.lower()) #put each word in the sentence within a list, j\n",
        "    return s\n",
        "\n",
        "#define function, clean_sentences, which removes punctuation and expands all contractions in a sentence\n",
        "def clean_sentences(text):\n",
        "    return remove_punctuation(expand_contractions(text))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOXNk319AXLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Expand contractions, remove punctuation all in one function clean_sentence\n",
        "GOT['Sentences_Clean'] = GOT.Sentence.apply(lambda x:clean_sentences(x))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS53Yh89CAnb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calculate words per line, assuming contractions are all expanded\n",
        "GOT[\"Num_Words\"] = GOT.Sentences_Clean.apply(lambda x: len(x.split()))\n",
        "GOT.to_csv (r'GOT_house_csv.csv', index = False, header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5ln7QIcDK6q",
        "colab_type": "text"
      },
      "source": [
        "# Now we will  prepare data for our Machine Learning Predictive model by only looking at msotly season 1 data and a subsection of season 2 data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdrjeoBTukgp",
        "colab_type": "code",
        "outputId": "c0720f49-f315-4bad-97bd-8562bb1ec0fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "#Extract only season one data and two data\n",
        "GOT1=GOT[GOT.Season==\"Season 1\"]\n",
        "GOT2=GOT[GOT.Season==\"Season 2\"]\n",
        "\n",
        "print(GOT1.head())\n",
        "print(GOT1.info())\n",
        "\n",
        "print(GOT2.head())\n",
        "print(GOT1.info())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Release Date  ... Num_Words\n",
            "0    4/17/2011  ...        27\n",
            "1    4/17/2011  ...        23\n",
            "2    4/17/2011  ...         5\n",
            "3    4/17/2011  ...         5\n",
            "4    4/17/2011  ...         7\n",
            "\n",
            "[5 rows x 9 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 3179 entries, 0 to 3178\n",
            "Data columns (total 9 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   Release Date     3179 non-null   object\n",
            " 1   Season           3179 non-null   object\n",
            " 2   Episode          3179 non-null   object\n",
            " 3   Episode Title    3179 non-null   object\n",
            " 4   Name             3179 non-null   object\n",
            " 5   Sentence         3179 non-null   object\n",
            " 6   House            2907 non-null   object\n",
            " 7   Sentences_Clean  3179 non-null   object\n",
            " 8   Num_Words        3179 non-null   int64 \n",
            "dtypes: int64(1), object(8)\n",
            "memory usage: 248.4+ KB\n",
            "None\n",
            "     Release Date    Season  ...                     Sentences_Clean Num_Words\n",
            "3179     4/1/2012  Season 2  ...        well struck… well struck dog         5\n",
            "3180     4/1/2012  Season 2  ...                   did you like that         4\n",
            "3181     4/1/2012  Season 2  ...       it was well struck your grace         6\n",
            "3182     4/1/2012  Season 2  ...   i already said it was well struck         7\n",
            "3183     4/1/2012  Season 2  ...                      yes your grace         3\n",
            "\n",
            "[5 rows x 9 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 3179 entries, 0 to 3178\n",
            "Data columns (total 9 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   Release Date     3179 non-null   object\n",
            " 1   Season           3179 non-null   object\n",
            " 2   Episode          3179 non-null   object\n",
            " 3   Episode Title    3179 non-null   object\n",
            " 4   Name             3179 non-null   object\n",
            " 5   Sentence         3179 non-null   object\n",
            " 6   House            2907 non-null   object\n",
            " 7   Sentences_Clean  3179 non-null   object\n",
            " 8   Num_Words        3179 non-null   int64 \n",
            "dtypes: int64(1), object(8)\n",
            "memory usage: 248.4+ KB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbaevQcMf3Qj",
        "colab_type": "code",
        "outputId": "05011d5a-09ae-449b-da2f-def3298c3292",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Keep character lines if and only if they still exist in season 2\n",
        "\n",
        "#Find a unique list of characters in season 2\n",
        "\n",
        "char_S2=GOT2.Name.unique()\n",
        "print(len(char_S2))\n",
        "\n",
        "#Filter S1 data if characters are in S2\n",
        "GOT1_modified=GOT1[GOT1['Name'].isin(char_S2)]\n",
        "print(len(GOT1_modified.Name.unique()))\n",
        "print(GOT1_modified.Name.unique())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "136\n",
            "49\n",
            "['jon snow' 'sansa stark' 'robb stark' 'catelyn stark' 'bran stark'\n",
            " 'theon greyjoy' 'jaime lannister' 'cersei lannister' 'luwin' 'arya stark'\n",
            " 'tyrion lannister' 'ros' 'daenerys targaryen' 'jorah mormont'\n",
            " 'khal drogo' 'sandor clegane' 'doreah' 'irri' 'joffrey lannister'\n",
            " 'myrcella baratheon' 'rodrick cassel' 'soldier' 'varys' 'renly baratheon'\n",
            " 'petyr baelish' 'grand maester pycelle' 'guard' 'jeor mormont' 'grenn'\n",
            " 'lancel lannister' 'rakharo' 'yoren' 'sam tarly' 'janos'\n",
            " 'gendry baratheon' 'bronn' 'loras tyrell' 'osha' 'wildling' 'man'\n",
            " 'tywin lannister' 'meryn trant' 'kevan lannister' 'all' 'prostitute'\n",
            " 'shae' 'rickon stark' 'karstark' 'hot pie']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUhNemrdcXNg",
        "colab_type": "code",
        "outputId": "b0901429-2328-46f0-a3e3-9e74439df9e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "#Further filter based on characters of interest \n",
        "#removing generic characters such as solider, guard, prostitute, etc\n",
        "important_names=np.delete(GOT1_modified.Name.unique(),(21,26,38,39,43,44));\n",
        "print(important_names)\n",
        "\n",
        "\n",
        "print(len(important_names))\n",
        "\n",
        "GOT1_modified=GOT1_modified[GOT1_modified['Name'].isin(important_names)]\n",
        "GOT1_modified.head()\n",
        "print(len(GOT1_modified.Name.unique()))\n",
        "\n",
        "GOT1_modified.info()\n",
        "GOT1=GOT1_modified\n",
        "GOT1.info()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['jon snow' 'sansa stark' 'robb stark' 'catelyn stark' 'bran stark'\n",
            " 'theon greyjoy' 'jaime lannister' 'cersei lannister' 'luwin' 'arya stark'\n",
            " 'tyrion lannister' 'ros' 'daenerys targaryen' 'jorah mormont'\n",
            " 'khal drogo' 'sandor clegane' 'doreah' 'irri' 'joffrey lannister'\n",
            " 'myrcella baratheon' 'rodrick cassel' 'varys' 'renly baratheon'\n",
            " 'petyr baelish' 'grand maester pycelle' 'jeor mormont' 'grenn'\n",
            " 'lancel lannister' 'rakharo' 'yoren' 'sam tarly' 'janos'\n",
            " 'gendry baratheon' 'bronn' 'loras tyrell' 'osha' 'tywin lannister'\n",
            " 'meryn trant' 'kevan lannister' 'shae' 'rickon stark' 'karstark'\n",
            " 'hot pie']\n",
            "43\n",
            "43\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 2120 entries, 15 to 3178\n",
            "Data columns (total 9 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   Release Date     2120 non-null   object\n",
            " 1   Season           2120 non-null   object\n",
            " 2   Episode          2120 non-null   object\n",
            " 3   Episode Title    2120 non-null   object\n",
            " 4   Name             2120 non-null   object\n",
            " 5   Sentence         2120 non-null   object\n",
            " 6   House            2081 non-null   object\n",
            " 7   Sentences_Clean  2120 non-null   object\n",
            " 8   Num_Words        2120 non-null   int64 \n",
            "dtypes: int64(1), object(8)\n",
            "memory usage: 165.6+ KB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 2120 entries, 15 to 3178\n",
            "Data columns (total 9 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   Release Date     2120 non-null   object\n",
            " 1   Season           2120 non-null   object\n",
            " 2   Episode          2120 non-null   object\n",
            " 3   Episode Title    2120 non-null   object\n",
            " 4   Name             2120 non-null   object\n",
            " 5   Sentence         2120 non-null   object\n",
            " 6   House            2081 non-null   object\n",
            " 7   Sentences_Clean  2120 non-null   object\n",
            " 8   Num_Words        2120 non-null   int64 \n",
            "dtypes: int64(1), object(8)\n",
            "memory usage: 165.6+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOo0mZPmOaqr",
        "colab_type": "code",
        "outputId": "fb3cc25e-5505-43fb-cc19-7c19d09b9a91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "#Tokenize the words and remove between words punctunations\n",
        "tokenizer=nltk.RegexpTokenizer(r\"\\w+\")\n",
        "GOT1['Tokenized_Sentence']=GOT1.Sentences_Clean.apply(lambda x:tokenizer.tokenize(x.lower()))\n",
        "GOT1.head(100)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Release Date</th>\n",
              "      <th>Season</th>\n",
              "      <th>Episode</th>\n",
              "      <th>Episode Title</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>House</th>\n",
              "      <th>Sentences_Clean</th>\n",
              "      <th>Num_Words</th>\n",
              "      <th>Tokenized_Sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>Go on. Father's watching.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>go on fathers watching</td>\n",
              "      <td>4</td>\n",
              "      <td>[go, on, fathers, watching]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>And your mother.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>and your mother</td>\n",
              "      <td>3</td>\n",
              "      <td>[and, your, mother]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>sansa stark</td>\n",
              "      <td>Thank you.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>thank you</td>\n",
              "      <td>2</td>\n",
              "      <td>[thank, you]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>Don't think too much, Bran.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>do not think too much bran</td>\n",
              "      <td>6</td>\n",
              "      <td>[do, not, think, too, much, bran]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>robb stark</td>\n",
              "      <td>Relax your bow arm.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>relax your bow arm</td>\n",
              "      <td>4</td>\n",
              "      <td>[relax, your, bow, arm]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>tyrion lannister</td>\n",
              "      <td>She has odd cravings, our sister.</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>she has odd cravings our sister</td>\n",
              "      <td>6</td>\n",
              "      <td>[she, has, odd, cravings, our, sister]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jaime lannister</td>\n",
              "      <td>A family trait. Now, the Starks are feasting u...</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>a family trait now the starks are feasting us...</td>\n",
              "      <td>19</td>\n",
              "      <td>[a, family, trait, now, the, starks, are, feas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>tyrion lannister</td>\n",
              "      <td>I'm sorry, I've begun the feast a bit early. A...</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>i am sorry i have begun the feast a bit early...</td>\n",
              "      <td>19</td>\n",
              "      <td>[i, am, sorry, i, have, begun, the, feast, a, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jaime lannister</td>\n",
              "      <td>I thought you might say that. But since we're ...</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>i thought you might say that but since we are...</td>\n",
              "      <td>20</td>\n",
              "      <td>[i, thought, you, might, say, that, but, since...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>tyrion lannister</td>\n",
              "      <td>Close the door!</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>close the door</td>\n",
              "      <td>3</td>\n",
              "      <td>[close, the, door]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Release Date  ...                                 Tokenized_Sentence\n",
              "15     4/17/2011  ...                        [go, on, fathers, watching]\n",
              "16     4/17/2011  ...                                [and, your, mother]\n",
              "18     4/17/2011  ...                                       [thank, you]\n",
              "21     4/17/2011  ...                  [do, not, think, too, much, bran]\n",
              "22     4/17/2011  ...                            [relax, your, bow, arm]\n",
              "..           ...  ...                                                ...\n",
              "163    4/17/2011  ...             [she, has, odd, cravings, our, sister]\n",
              "164    4/17/2011  ...  [a, family, trait, now, the, starks, are, feas...\n",
              "165    4/17/2011  ...  [i, am, sorry, i, have, begun, the, feast, a, ...\n",
              "166    4/17/2011  ...  [i, thought, you, might, say, that, but, since...\n",
              "167    4/17/2011  ...                                 [close, the, door]\n",
              "\n",
              "[100 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyBF3RVqPFf1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Remove stopwords for sentiment analysis\n",
        "stopword=nltk.corpus.stopwords.words('english')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J8WRWnGeFvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define a function to remove stop words\n",
        "def remove_stopwords(tokenized_sentence):\n",
        "  text=[word for word in tokenized_sentence if word not in stopword]\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGBoLfGGefi_",
        "colab_type": "code",
        "outputId": "33f1d5d4-0d77-4cf7-d07c-d33c9b5853fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#Implement the remove stop words function to a new column \n",
        "GOT1['Tokenized_No_Stop']=GOT1.Tokenized_Sentence.apply(lambda x:remove_stopwords(x))\n",
        "GOT1.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Release Date</th>\n",
              "      <th>Season</th>\n",
              "      <th>Episode</th>\n",
              "      <th>Episode Title</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>House</th>\n",
              "      <th>Sentences_Clean</th>\n",
              "      <th>Num_Words</th>\n",
              "      <th>Tokenized_Sentence</th>\n",
              "      <th>Tokenized_No_Stop</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>Go on. Father's watching.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>go on fathers watching</td>\n",
              "      <td>4</td>\n",
              "      <td>[go, on, fathers, watching]</td>\n",
              "      <td>[go, fathers, watching]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>And your mother.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>and your mother</td>\n",
              "      <td>3</td>\n",
              "      <td>[and, your, mother]</td>\n",
              "      <td>[mother]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>sansa stark</td>\n",
              "      <td>Thank you.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>thank you</td>\n",
              "      <td>2</td>\n",
              "      <td>[thank, you]</td>\n",
              "      <td>[thank]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>Don't think too much, Bran.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>do not think too much bran</td>\n",
              "      <td>6</td>\n",
              "      <td>[do, not, think, too, much, bran]</td>\n",
              "      <td>[think, much, bran]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>robb stark</td>\n",
              "      <td>Relax your bow arm.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>relax your bow arm</td>\n",
              "      <td>4</td>\n",
              "      <td>[relax, your, bow, arm]</td>\n",
              "      <td>[relax, bow, arm]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Release Date  ...        Tokenized_No_Stop\n",
              "15    4/17/2011  ...  [go, fathers, watching]\n",
              "16    4/17/2011  ...                 [mother]\n",
              "18    4/17/2011  ...                  [thank]\n",
              "21    4/17/2011  ...      [think, much, bran]\n",
              "22    4/17/2011  ...        [relax, bow, arm]\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnF0bKWXe63D",
        "colab_type": "code",
        "outputId": "00c00c12-3587-467d-941b-937dec40d968",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "# Stemming of the Non Stop Words Column\n",
        "wn=nltk.WordNetLemmatizer()\n",
        "\n",
        "def stem_reduction(tokenized_sentence):\n",
        "  sentence=[wn.lemmatize(word) for word in tokenized_sentence]\n",
        "  return (sentence)\n",
        "\n",
        "GOT1['Stemmed_Sentence']=GOT1['Tokenized_No_Stop'].apply(lambda x:stem_reduction(x))\n",
        "GOT1.tail()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Release Date</th>\n",
              "      <th>Season</th>\n",
              "      <th>Episode</th>\n",
              "      <th>Episode Title</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>House</th>\n",
              "      <th>Sentences_Clean</th>\n",
              "      <th>Num_Words</th>\n",
              "      <th>Tokenized_Sentence</th>\n",
              "      <th>Tokenized_No_Stop</th>\n",
              "      <th>Stemmed_Sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3173</th>\n",
              "      <td>6/19/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 10</td>\n",
              "      <td>Fire and Blood</td>\n",
              "      <td>jorah mormont</td>\n",
              "      <td>Don't ask me to stand aside as you climb on th...</td>\n",
              "      <td>House Mormont</td>\n",
              "      <td>do not ask me to stand aside as you climb on ...</td>\n",
              "      <td>19</td>\n",
              "      <td>[do, not, ask, me, to, stand, aside, as, you, ...</td>\n",
              "      <td>[ask, stand, aside, climb, pyre, l, watch, burn]</td>\n",
              "      <td>[ask, stand, aside, climb, pyre, l, watch, burn]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3174</th>\n",
              "      <td>6/19/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 10</td>\n",
              "      <td>Fire and Blood</td>\n",
              "      <td>daenerys targaryen</td>\n",
              "      <td>ls that what you fear? You will be my khalasar...</td>\n",
              "      <td>House Targaryen</td>\n",
              "      <td>ls that what you fear you will be my khalasar...</td>\n",
              "      <td>47</td>\n",
              "      <td>[ls, that, what, you, fear, you, will, be, my,...</td>\n",
              "      <td>[ls, fear, khalasar, l, see, faces, slaves, l,...</td>\n",
              "      <td>[l, fear, khalasar, l, see, face, slave, l, fr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3175</th>\n",
              "      <td>6/19/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 10</td>\n",
              "      <td>Fire and Blood</td>\n",
              "      <td>daenerys targaryen</td>\n",
              "      <td>Ser Jorah, bind this woman to the pyre. You sw...</td>\n",
              "      <td>House Targaryen</td>\n",
              "      <td>ser jorah bind this woman to the pyre you swo...</td>\n",
              "      <td>13</td>\n",
              "      <td>[ser, jorah, bind, this, woman, to, the, pyre,...</td>\n",
              "      <td>[ser, jorah, bind, woman, pyre, swore, obey]</td>\n",
              "      <td>[ser, jorah, bind, woman, pyre, swore, obey]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3177</th>\n",
              "      <td>6/19/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 10</td>\n",
              "      <td>Fire and Blood</td>\n",
              "      <td>daenerys targaryen</td>\n",
              "      <td>I will. But it is not your screams I want. Onl...</td>\n",
              "      <td>House Targaryen</td>\n",
              "      <td>i will but it is not your screams i want only...</td>\n",
              "      <td>13</td>\n",
              "      <td>[i, will, but, it, is, not, your, screams, i, ...</td>\n",
              "      <td>[screams, want, life]</td>\n",
              "      <td>[scream, want, life]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3178</th>\n",
              "      <td>6/19/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 10</td>\n",
              "      <td>Fire and Blood</td>\n",
              "      <td>jorah mormont</td>\n",
              "      <td>Blood of my blood.</td>\n",
              "      <td>House Mormont</td>\n",
              "      <td>blood of my blood</td>\n",
              "      <td>4</td>\n",
              "      <td>[blood, of, my, blood]</td>\n",
              "      <td>[blood, blood]</td>\n",
              "      <td>[blood, blood]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Release Date  ...                                   Stemmed_Sentence\n",
              "3173    6/19/2011  ...   [ask, stand, aside, climb, pyre, l, watch, burn]\n",
              "3174    6/19/2011  ...  [l, fear, khalasar, l, see, face, slave, l, fr...\n",
              "3175    6/19/2011  ...       [ser, jorah, bind, woman, pyre, swore, obey]\n",
              "3177    6/19/2011  ...                               [scream, want, life]\n",
              "3178    6/19/2011  ...                                     [blood, blood]\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzKLVQ4f9bRK",
        "colab_type": "code",
        "outputId": "35e0c3dc-ea6d-4fff-ec0e-d428ac10f0e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#Join the Tokenized Stemmed_Sentence into a string of sentence for the VADER sentiment analysis\n",
        "\n",
        "def convert_sentence (tokenized_sentence):\n",
        "  sentence=''\n",
        "  for word in tokenized_sentence:\n",
        "    sentence=sentence+\" \"+word\n",
        "  return sentence\n",
        "\n",
        "GOT1['Stemmed_Sentence_Non_Token']=GOT1['Stemmed_Sentence'].apply(lambda x:convert_sentence(x))\n",
        "\n",
        "GOT1['Stemmed_Sentence_Non_Token'].head()\n",
        "\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15     go father watching\n",
              "16                 mother\n",
              "18                  thank\n",
              "21        think much bran\n",
              "22          relax bow arm\n",
              "Name: Stemmed_Sentence_Non_Token, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek4IbNjs6ob5",
        "colab_type": "code",
        "outputId": "8e0ac971-9720-4139-8ce1-c3fa3132e17b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 980
        }
      },
      "source": [
        "#Use Vader Sentiment analysis tool \n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sid=SentimentIntensityAnalyzer()\n",
        "\n",
        "GOT1['Sentiment']=GOT1['Stemmed_Sentence_Non_Token'].apply(lambda x:sid.polarity_scores(x))\n",
        "\n",
        "def compound_score(Sentiment):\n",
        "  return(Sentiment['compound'])\n",
        "\n",
        "GOT1['Sentiment_Compound_Score']=GOT1['Sentiment'].apply(lambda x:compound_score(x))\n",
        "GOT1.head(100)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Release Date</th>\n",
              "      <th>Season</th>\n",
              "      <th>Episode</th>\n",
              "      <th>Episode Title</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>House</th>\n",
              "      <th>Sentences_Clean</th>\n",
              "      <th>Num_Words</th>\n",
              "      <th>Tokenized_Sentence</th>\n",
              "      <th>Tokenized_No_Stop</th>\n",
              "      <th>Stemmed_Sentence</th>\n",
              "      <th>Stemmed_Sentence_Non_Token</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Sentiment_Compound_Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>Go on. Father's watching.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>go on fathers watching</td>\n",
              "      <td>4</td>\n",
              "      <td>[go, on, fathers, watching]</td>\n",
              "      <td>[go, fathers, watching]</td>\n",
              "      <td>[go, father, watching]</td>\n",
              "      <td>go father watching</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>And your mother.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>and your mother</td>\n",
              "      <td>3</td>\n",
              "      <td>[and, your, mother]</td>\n",
              "      <td>[mother]</td>\n",
              "      <td>[mother]</td>\n",
              "      <td>mother</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>sansa stark</td>\n",
              "      <td>Thank you.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>thank you</td>\n",
              "      <td>2</td>\n",
              "      <td>[thank, you]</td>\n",
              "      <td>[thank]</td>\n",
              "      <td>[thank]</td>\n",
              "      <td>thank</td>\n",
              "      <td>{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound...</td>\n",
              "      <td>0.3612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>Don't think too much, Bran.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>do not think too much bran</td>\n",
              "      <td>6</td>\n",
              "      <td>[do, not, think, too, much, bran]</td>\n",
              "      <td>[think, much, bran]</td>\n",
              "      <td>[think, much, bran]</td>\n",
              "      <td>think much bran</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>robb stark</td>\n",
              "      <td>Relax your bow arm.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>relax your bow arm</td>\n",
              "      <td>4</td>\n",
              "      <td>[relax, your, bow, arm]</td>\n",
              "      <td>[relax, bow, arm]</td>\n",
              "      <td>[relax, bow, arm]</td>\n",
              "      <td>relax bow arm</td>\n",
              "      <td>{'neg': 0.0, 'neu': 0.408, 'pos': 0.592, 'comp...</td>\n",
              "      <td>0.4404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>tyrion lannister</td>\n",
              "      <td>She has odd cravings, our sister.</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>she has odd cravings our sister</td>\n",
              "      <td>6</td>\n",
              "      <td>[she, has, odd, cravings, our, sister]</td>\n",
              "      <td>[odd, cravings, sister]</td>\n",
              "      <td>[odd, craving, sister]</td>\n",
              "      <td>odd craving sister</td>\n",
              "      <td>{'neg': 0.535, 'neu': 0.465, 'pos': 0.0, 'comp...</td>\n",
              "      <td>-0.3182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jaime lannister</td>\n",
              "      <td>A family trait. Now, the Starks are feasting u...</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>a family trait now the starks are feasting us...</td>\n",
              "      <td>19</td>\n",
              "      <td>[a, family, trait, now, the, starks, are, feas...</td>\n",
              "      <td>[family, trait, starks, feasting, us, sundown,...</td>\n",
              "      <td>[family, trait, starks, feasting, u, sundown, ...</td>\n",
              "      <td>family trait starks feasting u sundown leave ...</td>\n",
              "      <td>{'neg': 0.348, 'neu': 0.652, 'pos': 0.0, 'comp...</td>\n",
              "      <td>-0.2960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>tyrion lannister</td>\n",
              "      <td>I'm sorry, I've begun the feast a bit early. A...</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>i am sorry i have begun the feast a bit early...</td>\n",
              "      <td>19</td>\n",
              "      <td>[i, am, sorry, i, have, begun, the, feast, a, ...</td>\n",
              "      <td>[sorry, begun, feast, bit, early, first, many,...</td>\n",
              "      <td>[sorry, begun, feast, bit, early, first, many,...</td>\n",
              "      <td>sorry begun feast bit early first many course</td>\n",
              "      <td>{'neg': 0.157, 'neu': 0.843, 'pos': 0.0, 'comp...</td>\n",
              "      <td>-0.0772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jaime lannister</td>\n",
              "      <td>I thought you might say that. But since we're ...</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>i thought you might say that but since we are...</td>\n",
              "      <td>20</td>\n",
              "      <td>[i, thought, you, might, say, that, but, since...</td>\n",
              "      <td>[thought, might, say, since, short, time, come...</td>\n",
              "      <td>[thought, might, say, since, short, time, come...</td>\n",
              "      <td>thought might say since short time come girl ...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>tyrion lannister</td>\n",
              "      <td>Close the door!</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>close the door</td>\n",
              "      <td>3</td>\n",
              "      <td>[close, the, door]</td>\n",
              "      <td>[close, door]</td>\n",
              "      <td>[close, door]</td>\n",
              "      <td>close door</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Release Date  ... Sentiment_Compound_Score\n",
              "15     4/17/2011  ...                   0.0000\n",
              "16     4/17/2011  ...                   0.0000\n",
              "18     4/17/2011  ...                   0.3612\n",
              "21     4/17/2011  ...                   0.0000\n",
              "22     4/17/2011  ...                   0.4404\n",
              "..           ...  ...                      ...\n",
              "163    4/17/2011  ...                  -0.3182\n",
              "164    4/17/2011  ...                  -0.2960\n",
              "165    4/17/2011  ...                  -0.0772\n",
              "166    4/17/2011  ...                   0.0000\n",
              "167    4/17/2011  ...                   0.0000\n",
              "\n",
              "[100 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yijZWz2Agxx-",
        "colab_type": "code",
        "outputId": "12a0c8e3-2660-411e-d50b-926261ddd58b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "#Set the character and house to category\n",
        "\n",
        "GOT1.info()\n",
        "GOT1['Name'].astype('category')\n",
        "GOT1['House'].astype('category')\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 2120 entries, 15 to 3178\n",
            "Data columns (total 15 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   Release Date                2120 non-null   object \n",
            " 1   Season                      2120 non-null   object \n",
            " 2   Episode                     2120 non-null   object \n",
            " 3   Episode Title               2120 non-null   object \n",
            " 4   Name                        2120 non-null   object \n",
            " 5   Sentence                    2120 non-null   object \n",
            " 6   House                       2081 non-null   object \n",
            " 7   Sentences_Clean             2120 non-null   object \n",
            " 8   Num_Words                   2120 non-null   int64  \n",
            " 9   Tokenized_Sentence          2120 non-null   object \n",
            " 10  Tokenized_No_Stop           2120 non-null   object \n",
            " 11  Stemmed_Sentence            2120 non-null   object \n",
            " 12  Stemmed_Sentence_Non_Token  2120 non-null   object \n",
            " 13  Sentiment                   2120 non-null   object \n",
            " 14  Sentiment_Compound_Score    2120 non-null   float64\n",
            "dtypes: float64(1), int64(1), object(13)\n",
            "memory usage: 265.0+ KB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15          House Stark\n",
              "16          House Stark\n",
              "18          House Stark\n",
              "21          House Stark\n",
              "22          House Stark\n",
              "             ...       \n",
              "3173      House Mormont\n",
              "3174    House Targaryen\n",
              "3175    House Targaryen\n",
              "3177    House Targaryen\n",
              "3178      House Mormont\n",
              "Name: House, Length: 2120, dtype: category\n",
              "Categories (17, object): [0, Dothraki, Free Folk, House Baelish, ..., House Tarly,\n",
              "                          House Trant, House Tyrell, Night's Watch]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aI8nrePfiUr7",
        "colab_type": "code",
        "outputId": "d0695bbc-296d-4d82-9284-99c3a34fc97e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        }
      },
      "source": [
        "#A Pivot Table to Show mean Compiund_Score by House\n",
        "table=pd.pivot_table(GOT1,values='Sentiment_Compound_Score',index=['House'],aggfunc=['mean','count'])\n",
        "table"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>mean</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>Sentiment_Compound_Score</th>\n",
              "      <th>Sentiment_Compound_Score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.098985</td>\n",
              "      <td>119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dothraki</th>\n",
              "      <td>-0.108385</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Free Folk</th>\n",
              "      <td>0.064064</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Baelish</th>\n",
              "      <td>0.126756</td>\n",
              "      <td>112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Baratheon</th>\n",
              "      <td>-0.064415</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Bronn</th>\n",
              "      <td>-0.003824</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Cassel</th>\n",
              "      <td>-0.127362</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Clegane</th>\n",
              "      <td>-0.095792</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Greyjoy</th>\n",
              "      <td>-0.038064</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Lannister</th>\n",
              "      <td>0.046368</td>\n",
              "      <td>564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Mormont</th>\n",
              "      <td>0.079796</td>\n",
              "      <td>117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Stark</th>\n",
              "      <td>0.011350</td>\n",
              "      <td>626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Targaryen</th>\n",
              "      <td>0.035417</td>\n",
              "      <td>189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Tarly</th>\n",
              "      <td>0.019794</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Trant</th>\n",
              "      <td>-0.282800</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Tyrell</th>\n",
              "      <td>-0.006087</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Night's Watch</th>\n",
              "      <td>-0.115121</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    mean                    count\n",
              "                Sentiment_Compound_Score Sentiment_Compound_Score\n",
              "House                                                            \n",
              "0                               0.098985                      119\n",
              "Dothraki                       -0.108385                       13\n",
              "Free Folk                       0.064064                       25\n",
              "House Baelish                   0.126756                      112\n",
              "House Baratheon                -0.064415                       54\n",
              "House Bronn                    -0.003824                       33\n",
              "House Cassel                   -0.127362                       13\n",
              "House Clegane                  -0.095792                       13\n",
              "House Greyjoy                  -0.038064                       70\n",
              "House Lannister                 0.046368                      564\n",
              "House Mormont                   0.079796                      117\n",
              "House Stark                     0.011350                      626\n",
              "House Targaryen                 0.035417                      189\n",
              "House Tarly                     0.019794                       54\n",
              "House Trant                    -0.282800                        4\n",
              "House Tyrell                   -0.006087                       23\n",
              "Night's Watch                  -0.115121                       52"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQekypR8oox3",
        "colab_type": "code",
        "outputId": "1d12ef96-fe06-47d7-ee27-28c92da01ae7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "#Plotting distributions \n",
        "House_Baelish=GOT1[GOT1.House=='House Baelish']\n",
        "House_Baratheon=GOT1[GOT1.House=='House Baratheon']\n",
        "House_Clegane=GOT1[GOT1.House=='House Clegane']\n",
        "House_Lannister=GOT1[GOT1.House=='House Lannister']\n",
        "House_Greyjoy=GOT1[GOT1.House=='House Greyjoy']\n",
        "House_Targaryen=GOT1[GOT1.House=='House Targaryen']\n",
        "House_Tyrell=GOT1[GOT1.House=='House Tyrell']\n",
        "House_Stark=GOT1[GOT1.House=='House Stark']\n",
        "House_Mormont=GOT1[GOT1.House=='House Mormont']\n",
        "Nights_Watch=GOT1[GOT1.House==\"Night's Watch\"]\n",
        "\n",
        "\n",
        "# Will only focus on the four main houses Lannister, Targaryen, Stark, and Mormont\n",
        "\n",
        "#sns.distplot(House_Baelish[['Sentiment_Compound_Score']], hist=False, kde=True,label='Baelish')\n",
        "#sns.distplot(House_Baratheon[['Sentiment_Compound_Score']], hist=False, kde=True,label='Baratheon')\n",
        "#sns.distplot(House_Clegane[['Sentiment_Compound_Score']], hist=False, kde=True,label='Clegane')\n",
        "#sns.distplot(House_Greyjoy[['Sentiment_Compound_Score']], hist=False,kde=True,label='Greyjoy')\n",
        "#sns.distplot(House_Tyrell[['Sentiment_Compound_Score']], hist=False, kde=True,label='Tyrell')\n",
        "#sns.distplot(Nights_Watch[['Sentiment_Compound_Score']], hist=False,kde=True,label='Nights_Watch')\n",
        "\n",
        "sns.distplot(House_Lannister[['Sentiment_Compound_Score']], hist=False, kde=True, label='Lannister')\n",
        "sns.distplot(House_Mormont[['Sentiment_Compound_Score']], hist=False, kde=True, label='Mormont')\n",
        "sns.distplot(House_Targaryen[['Sentiment_Compound_Score']], hist=False,kde=True,label='Targaryen')\n",
        "sns.distplot(House_Stark[['Sentiment_Compound_Score']], hist=False, kde=True,label='Stark')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fb14946f7b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3XCAm2HZ0Z7",
        "colab_type": "text"
      },
      "source": [
        "## As can seen from the above, there doesn't seem to be clear distinct separation in the sentiment distribution. But there is some standard deviation differences between them. We will analyze the standard deviation below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFaYifIectKL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "531879ad-3f80-48df-9cca-b0c763133e27"
      },
      "source": [
        "#A Pivot Table to Show mean and standard deviation Compiund_Score by House only for the four houses of interest\n",
        "\n",
        "table=pd.pivot_table(GOT1[GOT1['House'].isin(['House Lannister', 'House Stark','House Targaryen', 'House Mormont'])],values='Sentiment_Compound_Score',index=['House'],aggfunc=['mean','std','count'])\n",
        "table"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>Sentiment_Compound_Score</th>\n",
              "      <th>Sentiment_Compound_Score</th>\n",
              "      <th>Sentiment_Compound_Score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>House Lannister</th>\n",
              "      <td>0.046368</td>\n",
              "      <td>0.406960</td>\n",
              "      <td>564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Mormont</th>\n",
              "      <td>0.079796</td>\n",
              "      <td>0.364912</td>\n",
              "      <td>117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Stark</th>\n",
              "      <td>0.011350</td>\n",
              "      <td>0.340461</td>\n",
              "      <td>626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Targaryen</th>\n",
              "      <td>0.035417</td>\n",
              "      <td>0.334294</td>\n",
              "      <td>189</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    mean  ...                    count\n",
              "                Sentiment_Compound_Score  ... Sentiment_Compound_Score\n",
              "House                                     ...                         \n",
              "House Lannister                 0.046368  ...                      564\n",
              "House Mormont                   0.079796  ...                      117\n",
              "House Stark                     0.011350  ...                      626\n",
              "House Targaryen                 0.035417  ...                      189\n",
              "\n",
              "[4 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on04_eTigxrS",
        "colab_type": "text"
      },
      "source": [
        "##House Lannister has the highest spread in sentiment compared to the other houses. However, they all seem to have relatively close spread.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sU4qnU14SqWo",
        "colab_type": "text"
      },
      "source": [
        "# Applying machine learning models on data set - Naive Bays, Decision Tree, and Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvfbYOlhSpbq",
        "colab_type": "code",
        "outputId": "e631ffd2-a138-4d96-b555-de9977ff53c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "GOT1['House'].value_counts()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "House Stark        626\n",
              "House Lannister    564\n",
              "House Targaryen    189\n",
              "0                  119\n",
              "House Mormont      117\n",
              "House Baelish      112\n",
              "House Greyjoy       70\n",
              "House Tarly         54\n",
              "House Baratheon     54\n",
              "Night's Watch       52\n",
              "House Bronn         33\n",
              "Free Folk           25\n",
              "House Tyrell        23\n",
              "House Cassel        13\n",
              "House Clegane       13\n",
              "Dothraki            13\n",
              "House Trant          4\n",
              "Name: House, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiKaw5oNXQxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Select a subset that only contains the top four house records in terms of records.\n",
        "house_ml_list=['House Stark','House Lannister','House Targaryen','House Mormont']\n",
        "GOT1_ML=GOT1[GOT1['House'].isin(house_ml_list)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWum89SfXpmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Prepare training and testing dataset\n",
        "NewSentence=GOT1_ML[\"Stemmed_Sentence_Non_Token\"].tolist()\n",
        "#create bag od words model -> rows represent each line, each word is one column\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#Make sure only keep the top 1500 most frequent words for analysis\n",
        "cv=CountVectorizer(max_features=1500)\n",
        "#Create X and y -> X should be a large array where (# of records,#of words),y should be the \"name\" in GOT1\n",
        "X=cv.fit_transform(NewSentence).toarray()\n",
        "y=GOT1_ML.iloc[:,6].values\n",
        "#Make training and test sets for Naive Bays\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6zkdNx1yQag",
        "colab_type": "text"
      },
      "source": [
        "Model One - Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5T14Ub3X5ZP",
        "colab_type": "code",
        "outputId": "d5d5d1f0-34a2-4c2f-f3de-c11d9bbad15f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Fitting Naive Bays into the training set\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "classifier1=GaussianNB()\n",
        "classifier1.fit(X_train,y_train)\n",
        "#Predicting the test set\n",
        "y_pred_nb = classifier1.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test,y_pred_nb)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4090909090909091"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmaC6depydD8",
        "colab_type": "text"
      },
      "source": [
        "Model Two - Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJISVqoOyaQR",
        "colab_type": "code",
        "outputId": "731541bc-cfe6-404b-d9a6-4d91ae1a95e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "classifier2=DecisionTreeClassifier(criterion='entropy',random_state=0)\n",
        "classifier2.fit(X_train,y_train)\n",
        "y_pred_tree = classifier2.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test,y_pred_tree)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4572192513368984"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5obIq_7cyszZ",
        "colab_type": "text"
      },
      "source": [
        "Model Three - Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xdSH3jtyrtE",
        "colab_type": "code",
        "outputId": "6a07fb72-9beb-411d-be9b-4bb29261ad4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier3=RandomForestClassifier(n_estimators=10,criterion='entropy',random_state=0)\n",
        "classifier3.fit(X_train,y_train)\n",
        "y_pred_random = classifier3.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test,y_pred_random)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.48663101604278075"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDpJDDTuy3Ay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}