{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GOT Project.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trong-shen/Game-of-Throne-Project/blob/master/GOT_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kvbsdq0-guNm",
        "colab_type": "text"
      },
      "source": [
        "Load the CSV file from Github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt3EaJ_AgV2I",
        "colab_type": "code",
        "outputId": "c6ea91c3-a40d-4ede-936f-a70324107b8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "\n",
        "\n",
        "GOT= pd.read_csv('https://raw.githubusercontent.com/trong-shen/Game-of-Throne-Project/master/Game_of_Thrones_Script_clean.csv')\n",
        "char_info=pd.read_csv('https://raw.githubusercontent.com/trong-shen/Game-of-Throne-Project/master/got_table.csv')\n",
        "print(len(GOT))\n",
        "\n",
        "#Extract only the house data from char_info\n",
        "House=char_info[['name','house']]\n",
        "\n",
        "#Created a function to apply globally to the data frame\n",
        "def return_house(name):\n",
        "  house_dict=dict(zip(House.name,House.house))\n",
        "  try: \n",
        "    house=house_dict[name]\n",
        "    return(house)\n",
        "  except KeyError:\n",
        "    return(float(\"Nan\"))\n",
        "\n",
        "# Apply the house dict function to the whole GOT dataframe\n",
        "GOT['House']=GOT['Name'].apply(lambda x:return_house(x))\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "23911\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9thUHG5FPCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT79c3JB_d23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#list of contractions and the expanded mapping used for cleaning the data\n",
        "CONTRACTION_MAP = {\n",
        "\"ain't\": \"is not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"I'd\": \"I would\",\n",
        "\"I'd've\": \"I would have\",\n",
        "\"I'll\": \"I will\",\n",
        "\"I'll've\": \"I will have\",\n",
        "\"I'm\": \"I am\",\n",
        "\"I've\": \"I have\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cle0cONgAULs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define a function, expand_contractions, which takes a string and expands all contractions within the string using contraction_map\n",
        "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
        "    expanded = ''\n",
        "    text = text.lower() #make all text lowercase\n",
        "    wordList = text.split() #put text into a list of words\n",
        "    for i in range(len(wordList)):\n",
        "        if wordList[i] in contraction_mapping.keys(): #for each word, if it is a contraction in the listing\n",
        "            expanded = expanded + ' ' + contraction_mapping[wordList[i]] #then replace with the expanded version\n",
        "        else:\n",
        "            expanded = expanded + ' ' + wordList[i] #otherwise, keep the original word\n",
        "    return expanded\n",
        "\n",
        "#define a function, remove_punctuation, which takes in a string and removes all punctuation \n",
        "def remove_punctuation(s):\n",
        "    s = s.translate(str.maketrans('','',string.punctuation)) #take out punctuation in the sentence\n",
        "    j = nltk.word_tokenize(s.lower()) #put each word in the sentence within a list, j\n",
        "    return s\n",
        "\n",
        "#define function, clean_sentences, which removes punctuation and expands all contractions in a sentence\n",
        "def clean_sentences(text):\n",
        "    return remove_punctuation(expand_contractions(text))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOXNk319AXLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Expand contractions, remove punctuation all in one function clean_sentence\n",
        "GOT['Sentences_Clean'] = GOT.Sentence.apply(lambda x:clean_sentences(x))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS53Yh89CAnb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calculate words per line, assuming contractions are all expanded\n",
        "GOT[\"Num_Words\"] = GOT.Sentences_Clean.apply(lambda x: len(x.split()))\n",
        "GOT.to_csv (r'GOT_house_csv.csv', index = False, header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5ln7QIcDK6q",
        "colab_type": "text"
      },
      "source": [
        "# Now we will  prepare data for our Machine Learning Predictive model by only looking at msotly season 1 data and a subsection of season 2 data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdrjeoBTukgp",
        "colab_type": "code",
        "outputId": "dd51281c-84d7-456a-a116-6ea754b8dd46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "#Extract only season one data and two data\n",
        "GOT1=GOT[GOT.Season==\"Season 1\"]\n",
        "GOT2=GOT[GOT.Season==\"Season 2\"]\n",
        "\n",
        "print(GOT1.head())\n",
        "print(GOT1.info())\n",
        "\n",
        "print(GOT2.head())\n",
        "print(GOT1.info())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Release Date  ... Num_Words\n",
            "0    4/17/2011  ...        27\n",
            "1    4/17/2011  ...        23\n",
            "2    4/17/2011  ...         5\n",
            "3    4/17/2011  ...         5\n",
            "4    4/17/2011  ...         7\n",
            "\n",
            "[5 rows x 9 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 3179 entries, 0 to 3178\n",
            "Data columns (total 9 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   Release Date     3179 non-null   object\n",
            " 1   Season           3179 non-null   object\n",
            " 2   Episode          3179 non-null   object\n",
            " 3   Episode Title    3179 non-null   object\n",
            " 4   Name             3179 non-null   object\n",
            " 5   Sentence         3179 non-null   object\n",
            " 6   House            2907 non-null   object\n",
            " 7   Sentences_Clean  3179 non-null   object\n",
            " 8   Num_Words        3179 non-null   int64 \n",
            "dtypes: int64(1), object(8)\n",
            "memory usage: 248.4+ KB\n",
            "None\n",
            "     Release Date    Season  ...                     Sentences_Clean Num_Words\n",
            "3179     4/1/2012  Season 2  ...        well struck… well struck dog         5\n",
            "3180     4/1/2012  Season 2  ...                   did you like that         4\n",
            "3181     4/1/2012  Season 2  ...       it was well struck your grace         6\n",
            "3182     4/1/2012  Season 2  ...   i already said it was well struck         7\n",
            "3183     4/1/2012  Season 2  ...                      yes your grace         3\n",
            "\n",
            "[5 rows x 9 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 3179 entries, 0 to 3178\n",
            "Data columns (total 9 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   Release Date     3179 non-null   object\n",
            " 1   Season           3179 non-null   object\n",
            " 2   Episode          3179 non-null   object\n",
            " 3   Episode Title    3179 non-null   object\n",
            " 4   Name             3179 non-null   object\n",
            " 5   Sentence         3179 non-null   object\n",
            " 6   House            2907 non-null   object\n",
            " 7   Sentences_Clean  3179 non-null   object\n",
            " 8   Num_Words        3179 non-null   int64 \n",
            "dtypes: int64(1), object(8)\n",
            "memory usage: 248.4+ KB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbaevQcMf3Qj",
        "colab_type": "code",
        "outputId": "4660a2fc-e995-49d7-bde5-d8249fa44aa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Keep character lines if and only if they still exist in season 2\n",
        "\n",
        "#Find a unique list of characters in season 2\n",
        "\n",
        "char_S2=GOT2.Name.unique()\n",
        "print(len(char_S2))\n",
        "\n",
        "#Filter S1 data if characters are in S2\n",
        "GOT1_modified=GOT1[GOT1['Name'].isin(char_S2)]\n",
        "print(len(GOT1_modified.Name.unique()))\n",
        "print(GOT1_modified.Name.unique())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "136\n",
            "49\n",
            "['jon snow' 'sansa stark' 'robb stark' 'catelyn stark' 'bran stark'\n",
            " 'theon greyjoy' 'jaime lannister' 'cersei lannister' 'luwin' 'arya stark'\n",
            " 'tyrion lannister' 'ros' 'daenerys targaryen' 'jorah mormont'\n",
            " 'khal drogo' 'sandor clegane' 'doreah' 'irri' 'joffrey lannister'\n",
            " 'myrcella baratheon' 'rodrick cassel' 'soldier' 'varys' 'renly baratheon'\n",
            " 'petyr baelish' 'grand maester pycelle' 'guard' 'jeor mormont' 'grenn'\n",
            " 'lancel lannister' 'rakharo' 'yoren' 'sam tarly' 'janos'\n",
            " 'gendry baratheon' 'bronn' 'loras tyrell' 'osha' 'wildling' 'man'\n",
            " 'tywin lannister' 'meryn trant' 'kevan lannister' 'all' 'prostitute'\n",
            " 'shae' 'rickon stark' 'karstark' 'hot pie']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUhNemrdcXNg",
        "colab_type": "code",
        "outputId": "e3e17963-30f4-4269-ac25-7aa4046fd2ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "#Further filter based on characters of interest \n",
        "#removing generic characters such as solider, guard, prostitute, etc\n",
        "important_names=np.delete(GOT1_modified.Name.unique(),(21,26,38,39,43,44));\n",
        "print(important_names)\n",
        "\n",
        "\n",
        "print(len(important_names))\n",
        "\n",
        "GOT1_modified=GOT1_modified[GOT1_modified['Name'].isin(important_names)]\n",
        "GOT1_modified.head()\n",
        "print(len(GOT1_modified.Name.unique()))\n",
        "\n",
        "GOT1_modified.info()\n",
        "GOT1=GOT1_modified\n",
        "GOT1.info()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['jon snow' 'sansa stark' 'robb stark' 'catelyn stark' 'bran stark'\n",
            " 'theon greyjoy' 'jaime lannister' 'cersei lannister' 'luwin' 'arya stark'\n",
            " 'tyrion lannister' 'ros' 'daenerys targaryen' 'jorah mormont'\n",
            " 'khal drogo' 'sandor clegane' 'doreah' 'irri' 'joffrey lannister'\n",
            " 'myrcella baratheon' 'rodrick cassel' 'varys' 'renly baratheon'\n",
            " 'petyr baelish' 'grand maester pycelle' 'jeor mormont' 'grenn'\n",
            " 'lancel lannister' 'rakharo' 'yoren' 'sam tarly' 'janos'\n",
            " 'gendry baratheon' 'bronn' 'loras tyrell' 'osha' 'tywin lannister'\n",
            " 'meryn trant' 'kevan lannister' 'shae' 'rickon stark' 'karstark'\n",
            " 'hot pie']\n",
            "43\n",
            "43\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 2120 entries, 15 to 3178\n",
            "Data columns (total 9 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   Release Date     2120 non-null   object\n",
            " 1   Season           2120 non-null   object\n",
            " 2   Episode          2120 non-null   object\n",
            " 3   Episode Title    2120 non-null   object\n",
            " 4   Name             2120 non-null   object\n",
            " 5   Sentence         2120 non-null   object\n",
            " 6   House            2081 non-null   object\n",
            " 7   Sentences_Clean  2120 non-null   object\n",
            " 8   Num_Words        2120 non-null   int64 \n",
            "dtypes: int64(1), object(8)\n",
            "memory usage: 165.6+ KB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 2120 entries, 15 to 3178\n",
            "Data columns (total 9 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   Release Date     2120 non-null   object\n",
            " 1   Season           2120 non-null   object\n",
            " 2   Episode          2120 non-null   object\n",
            " 3   Episode Title    2120 non-null   object\n",
            " 4   Name             2120 non-null   object\n",
            " 5   Sentence         2120 non-null   object\n",
            " 6   House            2081 non-null   object\n",
            " 7   Sentences_Clean  2120 non-null   object\n",
            " 8   Num_Words        2120 non-null   int64 \n",
            "dtypes: int64(1), object(8)\n",
            "memory usage: 165.6+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOo0mZPmOaqr",
        "colab_type": "code",
        "outputId": "6839d8ad-a938-43a4-d654-cd8373fbeba0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 983
        }
      },
      "source": [
        "#Tokenize the words and remove between words punctunations\n",
        "tokenizer=nltk.RegexpTokenizer(r\"\\w+\")\n",
        "GOT1['Tokenized_Sentence']=GOT1.Sentences_Clean.apply(lambda x:tokenizer.tokenize(x.lower()))\n",
        "GOT1.head(100)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Release Date</th>\n",
              "      <th>Season</th>\n",
              "      <th>Episode</th>\n",
              "      <th>Episode Title</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>House</th>\n",
              "      <th>Sentences_Clean</th>\n",
              "      <th>Num_Words</th>\n",
              "      <th>Tokenized_Sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>Go on. Father's watching.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>go on fathers watching</td>\n",
              "      <td>4</td>\n",
              "      <td>[go, on, fathers, watching]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>And your mother.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>and your mother</td>\n",
              "      <td>3</td>\n",
              "      <td>[and, your, mother]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>sansa stark</td>\n",
              "      <td>Thank you.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>thank you</td>\n",
              "      <td>2</td>\n",
              "      <td>[thank, you]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>Don't think too much, Bran.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>do not think too much bran</td>\n",
              "      <td>6</td>\n",
              "      <td>[do, not, think, too, much, bran]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>robb stark</td>\n",
              "      <td>Relax your bow arm.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>relax your bow arm</td>\n",
              "      <td>4</td>\n",
              "      <td>[relax, your, bow, arm]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>tyrion lannister</td>\n",
              "      <td>She has odd cravings, our sister.</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>she has odd cravings our sister</td>\n",
              "      <td>6</td>\n",
              "      <td>[she, has, odd, cravings, our, sister]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jaime lannister</td>\n",
              "      <td>A family trait. Now, the Starks are feasting u...</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>a family trait now the starks are feasting us...</td>\n",
              "      <td>19</td>\n",
              "      <td>[a, family, trait, now, the, starks, are, feas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>tyrion lannister</td>\n",
              "      <td>I'm sorry, I've begun the feast a bit early. A...</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>i am sorry i have begun the feast a bit early...</td>\n",
              "      <td>19</td>\n",
              "      <td>[i, am, sorry, i, have, begun, the, feast, a, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jaime lannister</td>\n",
              "      <td>I thought you might say that. But since we're ...</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>i thought you might say that but since we are...</td>\n",
              "      <td>20</td>\n",
              "      <td>[i, thought, you, might, say, that, but, since...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>tyrion lannister</td>\n",
              "      <td>Close the door!</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>close the door</td>\n",
              "      <td>3</td>\n",
              "      <td>[close, the, door]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Release Date  ...                                 Tokenized_Sentence\n",
              "15     4/17/2011  ...                        [go, on, fathers, watching]\n",
              "16     4/17/2011  ...                                [and, your, mother]\n",
              "18     4/17/2011  ...                                       [thank, you]\n",
              "21     4/17/2011  ...                  [do, not, think, too, much, bran]\n",
              "22     4/17/2011  ...                            [relax, your, bow, arm]\n",
              "..           ...  ...                                                ...\n",
              "163    4/17/2011  ...             [she, has, odd, cravings, our, sister]\n",
              "164    4/17/2011  ...  [a, family, trait, now, the, starks, are, feas...\n",
              "165    4/17/2011  ...  [i, am, sorry, i, have, begun, the, feast, a, ...\n",
              "166    4/17/2011  ...  [i, thought, you, might, say, that, but, since...\n",
              "167    4/17/2011  ...                                 [close, the, door]\n",
              "\n",
              "[100 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyBF3RVqPFf1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Remove stopwords for sentiment analysis\n",
        "stopword=nltk.corpus.stopwords.words('english')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J8WRWnGeFvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define a function to remove stop words\n",
        "def remove_stopwords(tokenized_sentence):\n",
        "  text=[word for word in tokenized_sentence if word not in stopword]\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGBoLfGGefi_",
        "colab_type": "code",
        "outputId": "ac32b29b-e59e-4297-a7ef-4a445e96af7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "#Implement the stop words function to a new column \n",
        "GOT1['Tokenized_No_Stop']=GOT1.Tokenized_Sentence.apply(lambda x:remove_stopwords(x))\n",
        "GOT1.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Release Date</th>\n",
              "      <th>Season</th>\n",
              "      <th>Episode</th>\n",
              "      <th>Episode Title</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>House</th>\n",
              "      <th>Sentences_Clean</th>\n",
              "      <th>Num_Words</th>\n",
              "      <th>Tokenized_Sentence</th>\n",
              "      <th>Tokenized_No_Stop</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>Go on. Father's watching.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>go on fathers watching</td>\n",
              "      <td>4</td>\n",
              "      <td>[go, on, fathers, watching]</td>\n",
              "      <td>[go, fathers, watching]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>And your mother.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>and your mother</td>\n",
              "      <td>3</td>\n",
              "      <td>[and, your, mother]</td>\n",
              "      <td>[mother]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>sansa stark</td>\n",
              "      <td>Thank you.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>thank you</td>\n",
              "      <td>2</td>\n",
              "      <td>[thank, you]</td>\n",
              "      <td>[thank]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>Don't think too much, Bran.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>do not think too much bran</td>\n",
              "      <td>6</td>\n",
              "      <td>[do, not, think, too, much, bran]</td>\n",
              "      <td>[think, much, bran]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>robb stark</td>\n",
              "      <td>Relax your bow arm.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>relax your bow arm</td>\n",
              "      <td>4</td>\n",
              "      <td>[relax, your, bow, arm]</td>\n",
              "      <td>[relax, bow, arm]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Release Date  ...        Tokenized_No_Stop\n",
              "15    4/17/2011  ...  [go, fathers, watching]\n",
              "16    4/17/2011  ...                 [mother]\n",
              "18    4/17/2011  ...                  [thank]\n",
              "21    4/17/2011  ...      [think, much, bran]\n",
              "22    4/17/2011  ...        [relax, bow, arm]\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnF0bKWXe63D",
        "colab_type": "code",
        "outputId": "d82c941d-2cc7-4210-c666-b2257a2ad5a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "# Stemming of the Non Stop Words Column\n",
        "wn=nltk.WordNetLemmatizer()\n",
        "\n",
        "def stem_reduction(tokenized_sentence):\n",
        "  sentence=[wn.lemmatize(word) for word in tokenized_sentence]\n",
        "  return (sentence)\n",
        "\n",
        "GOT1['Stemmed_Sentence']=GOT1['Tokenized_No_Stop'].apply(lambda x:stem_reduction(x))\n",
        "GOT1.tail()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Release Date</th>\n",
              "      <th>Season</th>\n",
              "      <th>Episode</th>\n",
              "      <th>Episode Title</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>House</th>\n",
              "      <th>Sentences_Clean</th>\n",
              "      <th>Num_Words</th>\n",
              "      <th>Tokenized_Sentence</th>\n",
              "      <th>Tokenized_No_Stop</th>\n",
              "      <th>Stemmed_Sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3173</th>\n",
              "      <td>6/19/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 10</td>\n",
              "      <td>Fire and Blood</td>\n",
              "      <td>jorah mormont</td>\n",
              "      <td>Don't ask me to stand aside as you climb on th...</td>\n",
              "      <td>House Mormont</td>\n",
              "      <td>do not ask me to stand aside as you climb on ...</td>\n",
              "      <td>19</td>\n",
              "      <td>[do, not, ask, me, to, stand, aside, as, you, ...</td>\n",
              "      <td>[ask, stand, aside, climb, pyre, l, watch, burn]</td>\n",
              "      <td>[ask, stand, aside, climb, pyre, l, watch, burn]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3174</th>\n",
              "      <td>6/19/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 10</td>\n",
              "      <td>Fire and Blood</td>\n",
              "      <td>daenerys targaryen</td>\n",
              "      <td>ls that what you fear? You will be my khalasar...</td>\n",
              "      <td>House Targaryen</td>\n",
              "      <td>ls that what you fear you will be my khalasar...</td>\n",
              "      <td>47</td>\n",
              "      <td>[ls, that, what, you, fear, you, will, be, my,...</td>\n",
              "      <td>[ls, fear, khalasar, l, see, faces, slaves, l,...</td>\n",
              "      <td>[l, fear, khalasar, l, see, face, slave, l, fr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3175</th>\n",
              "      <td>6/19/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 10</td>\n",
              "      <td>Fire and Blood</td>\n",
              "      <td>daenerys targaryen</td>\n",
              "      <td>Ser Jorah, bind this woman to the pyre. You sw...</td>\n",
              "      <td>House Targaryen</td>\n",
              "      <td>ser jorah bind this woman to the pyre you swo...</td>\n",
              "      <td>13</td>\n",
              "      <td>[ser, jorah, bind, this, woman, to, the, pyre,...</td>\n",
              "      <td>[ser, jorah, bind, woman, pyre, swore, obey]</td>\n",
              "      <td>[ser, jorah, bind, woman, pyre, swore, obey]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3177</th>\n",
              "      <td>6/19/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 10</td>\n",
              "      <td>Fire and Blood</td>\n",
              "      <td>daenerys targaryen</td>\n",
              "      <td>I will. But it is not your screams I want. Onl...</td>\n",
              "      <td>House Targaryen</td>\n",
              "      <td>i will but it is not your screams i want only...</td>\n",
              "      <td>13</td>\n",
              "      <td>[i, will, but, it, is, not, your, screams, i, ...</td>\n",
              "      <td>[screams, want, life]</td>\n",
              "      <td>[scream, want, life]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3178</th>\n",
              "      <td>6/19/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 10</td>\n",
              "      <td>Fire and Blood</td>\n",
              "      <td>jorah mormont</td>\n",
              "      <td>Blood of my blood.</td>\n",
              "      <td>House Mormont</td>\n",
              "      <td>blood of my blood</td>\n",
              "      <td>4</td>\n",
              "      <td>[blood, of, my, blood]</td>\n",
              "      <td>[blood, blood]</td>\n",
              "      <td>[blood, blood]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Release Date  ...                                   Stemmed_Sentence\n",
              "3173    6/19/2011  ...   [ask, stand, aside, climb, pyre, l, watch, burn]\n",
              "3174    6/19/2011  ...  [l, fear, khalasar, l, see, face, slave, l, fr...\n",
              "3175    6/19/2011  ...       [ser, jorah, bind, woman, pyre, swore, obey]\n",
              "3177    6/19/2011  ...                               [scream, want, life]\n",
              "3178    6/19/2011  ...                                     [blood, blood]\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzKLVQ4f9bRK",
        "colab_type": "code",
        "outputId": "9e6fb19c-9567-494f-89bc-50fec4afffe8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#Join the Tokenized Stemmed_Sentence into a string of sentence for the VADER sentiment analysis\n",
        "\n",
        "def convert_sentence (tokenized_sentence):\n",
        "  sentence=''\n",
        "  for word in tokenized_sentence:\n",
        "    sentence=sentence+\" \"+word\n",
        "  return sentence\n",
        "\n",
        "GOT1['Stemmed_Sentence_Non_Token']=GOT1['Stemmed_Sentence'].apply(lambda x:convert_sentence(x))\n",
        "\n",
        "GOT1['Stemmed_Sentence_Non_Token'].head()\n",
        "\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15     go father watching\n",
              "16                 mother\n",
              "18                  thank\n",
              "21        think much bran\n",
              "22          relax bow arm\n",
              "Name: Stemmed_Sentence_Non_Token, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek4IbNjs6ob5",
        "colab_type": "code",
        "outputId": "a6ab8f0e-6578-43ee-ab38-e75c67e482b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Use Vader Sentiment analysis tool \n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sid=SentimentIntensityAnalyzer()\n",
        "\n",
        "GOT1['Sentiment']=GOT1['Stemmed_Sentence_Non_Token'].apply(lambda x:sid.polarity_scores(x))\n",
        "\n",
        "def compound_score(Sentiment):\n",
        "  return(Sentiment['compound'])\n",
        "\n",
        "GOT1['Sentiment_Compound_Score']=GOT1['Sentiment'].apply(lambda x:compound_score(x))\n",
        "GOT1.head(100)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Release Date</th>\n",
              "      <th>Season</th>\n",
              "      <th>Episode</th>\n",
              "      <th>Episode Title</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>House</th>\n",
              "      <th>Sentences_Clean</th>\n",
              "      <th>Num_Words</th>\n",
              "      <th>Tokenized_Sentence</th>\n",
              "      <th>Tokenized_No_Stop</th>\n",
              "      <th>Stemmed_Sentence</th>\n",
              "      <th>Stemmed_Sentence_Non_Token</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Sentiment_Compound_Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>Go on. Father's watching.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>go on fathers watching</td>\n",
              "      <td>4</td>\n",
              "      <td>[go, on, fathers, watching]</td>\n",
              "      <td>[go, fathers, watching]</td>\n",
              "      <td>[go, father, watching]</td>\n",
              "      <td>go father watching</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>And your mother.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>and your mother</td>\n",
              "      <td>3</td>\n",
              "      <td>[and, your, mother]</td>\n",
              "      <td>[mother]</td>\n",
              "      <td>[mother]</td>\n",
              "      <td>mother</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>sansa stark</td>\n",
              "      <td>Thank you.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>thank you</td>\n",
              "      <td>2</td>\n",
              "      <td>[thank, you]</td>\n",
              "      <td>[thank]</td>\n",
              "      <td>[thank]</td>\n",
              "      <td>thank</td>\n",
              "      <td>{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound...</td>\n",
              "      <td>0.3612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>Don't think too much, Bran.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>do not think too much bran</td>\n",
              "      <td>6</td>\n",
              "      <td>[do, not, think, too, much, bran]</td>\n",
              "      <td>[think, much, bran]</td>\n",
              "      <td>[think, much, bran]</td>\n",
              "      <td>think much bran</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>robb stark</td>\n",
              "      <td>Relax your bow arm.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>relax your bow arm</td>\n",
              "      <td>4</td>\n",
              "      <td>[relax, your, bow, arm]</td>\n",
              "      <td>[relax, bow, arm]</td>\n",
              "      <td>[relax, bow, arm]</td>\n",
              "      <td>relax bow arm</td>\n",
              "      <td>{'neg': 0.0, 'neu': 0.408, 'pos': 0.592, 'comp...</td>\n",
              "      <td>0.4404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>tyrion lannister</td>\n",
              "      <td>She has odd cravings, our sister.</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>she has odd cravings our sister</td>\n",
              "      <td>6</td>\n",
              "      <td>[she, has, odd, cravings, our, sister]</td>\n",
              "      <td>[odd, cravings, sister]</td>\n",
              "      <td>[odd, craving, sister]</td>\n",
              "      <td>odd craving sister</td>\n",
              "      <td>{'neg': 0.535, 'neu': 0.465, 'pos': 0.0, 'comp...</td>\n",
              "      <td>-0.3182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jaime lannister</td>\n",
              "      <td>A family trait. Now, the Starks are feasting u...</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>a family trait now the starks are feasting us...</td>\n",
              "      <td>19</td>\n",
              "      <td>[a, family, trait, now, the, starks, are, feas...</td>\n",
              "      <td>[family, trait, starks, feasting, us, sundown,...</td>\n",
              "      <td>[family, trait, starks, feasting, u, sundown, ...</td>\n",
              "      <td>family trait starks feasting u sundown leave ...</td>\n",
              "      <td>{'neg': 0.348, 'neu': 0.652, 'pos': 0.0, 'comp...</td>\n",
              "      <td>-0.2960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>tyrion lannister</td>\n",
              "      <td>I'm sorry, I've begun the feast a bit early. A...</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>i am sorry i have begun the feast a bit early...</td>\n",
              "      <td>19</td>\n",
              "      <td>[i, am, sorry, i, have, begun, the, feast, a, ...</td>\n",
              "      <td>[sorry, begun, feast, bit, early, first, many,...</td>\n",
              "      <td>[sorry, begun, feast, bit, early, first, many,...</td>\n",
              "      <td>sorry begun feast bit early first many course</td>\n",
              "      <td>{'neg': 0.157, 'neu': 0.843, 'pos': 0.0, 'comp...</td>\n",
              "      <td>-0.0772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jaime lannister</td>\n",
              "      <td>I thought you might say that. But since we're ...</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>i thought you might say that but since we are...</td>\n",
              "      <td>20</td>\n",
              "      <td>[i, thought, you, might, say, that, but, since...</td>\n",
              "      <td>[thought, might, say, since, short, time, come...</td>\n",
              "      <td>[thought, might, say, since, short, time, come...</td>\n",
              "      <td>thought might say since short time come girl ...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>tyrion lannister</td>\n",
              "      <td>Close the door!</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>close the door</td>\n",
              "      <td>3</td>\n",
              "      <td>[close, the, door]</td>\n",
              "      <td>[close, door]</td>\n",
              "      <td>[close, door]</td>\n",
              "      <td>close door</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Release Date  ... Sentiment_Compound_Score\n",
              "15     4/17/2011  ...                   0.0000\n",
              "16     4/17/2011  ...                   0.0000\n",
              "18     4/17/2011  ...                   0.3612\n",
              "21     4/17/2011  ...                   0.0000\n",
              "22     4/17/2011  ...                   0.4404\n",
              "..           ...  ...                      ...\n",
              "163    4/17/2011  ...                  -0.3182\n",
              "164    4/17/2011  ...                  -0.2960\n",
              "165    4/17/2011  ...                  -0.0772\n",
              "166    4/17/2011  ...                   0.0000\n",
              "167    4/17/2011  ...                   0.0000\n",
              "\n",
              "[100 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yijZWz2Agxx-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "cae47e55-aa27-40b1-ebee-4482732751fc"
      },
      "source": [
        "#Set the character and house to category\n",
        "\n",
        "GOT1.info()\n",
        "GOT1['Name'].astype('category')\n",
        "GOT1['House'].astype('category')\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 2120 entries, 15 to 3178\n",
            "Data columns (total 15 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   Release Date                2120 non-null   object \n",
            " 1   Season                      2120 non-null   object \n",
            " 2   Episode                     2120 non-null   object \n",
            " 3   Episode Title               2120 non-null   object \n",
            " 4   Name                        2120 non-null   object \n",
            " 5   Sentence                    2120 non-null   object \n",
            " 6   House                       2081 non-null   object \n",
            " 7   Sentences_Clean             2120 non-null   object \n",
            " 8   Num_Words                   2120 non-null   int64  \n",
            " 9   Tokenized_Sentence          2120 non-null   object \n",
            " 10  Tokenized_No_Stop           2120 non-null   object \n",
            " 11  Stemmed_Sentence            2120 non-null   object \n",
            " 12  Stemmed_Sentence_Non_Token  2120 non-null   object \n",
            " 13  Sentiment                   2120 non-null   object \n",
            " 14  Sentiment_Compound_Score    2120 non-null   float64\n",
            "dtypes: float64(1), int64(1), object(13)\n",
            "memory usage: 345.0+ KB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15          House Stark\n",
              "16          House Stark\n",
              "18          House Stark\n",
              "21          House Stark\n",
              "22          House Stark\n",
              "             ...       \n",
              "3173      House Mormont\n",
              "3174    House Targaryen\n",
              "3175    House Targaryen\n",
              "3177    House Targaryen\n",
              "3178      House Mormont\n",
              "Name: House, Length: 2120, dtype: category\n",
              "Categories (17, object): [0, Dothraki, Free Folk, House Baelish, ..., House Tarly,\n",
              "                          House Trant, House Tyrell, Night's Watch]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aI8nrePfiUr7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "668b55da-ae4f-4b52-8c7e-5c372d936e8b"
      },
      "source": [
        "#A Pivot Table to Show mean Compiund_Score by House\n",
        "table=pd.pivot_table(GOT1,values='Sentiment_Compound_Score',index=['House'],aggfunc='mean')\n",
        "table"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment_Compound_Score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.098985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dothraki</th>\n",
              "      <td>-0.108385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Free Folk</th>\n",
              "      <td>0.064064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Baelish</th>\n",
              "      <td>0.126756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Baratheon</th>\n",
              "      <td>-0.064415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Bronn</th>\n",
              "      <td>-0.003824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Cassel</th>\n",
              "      <td>-0.127362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Clegane</th>\n",
              "      <td>-0.095792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Greyjoy</th>\n",
              "      <td>-0.038064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Lannister</th>\n",
              "      <td>0.046368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Mormont</th>\n",
              "      <td>0.079796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Stark</th>\n",
              "      <td>0.011350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Targaryen</th>\n",
              "      <td>0.035417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Tarly</th>\n",
              "      <td>0.019794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Trant</th>\n",
              "      <td>-0.282800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Tyrell</th>\n",
              "      <td>-0.006087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Night's Watch</th>\n",
              "      <td>-0.115121</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Sentiment_Compound_Score\n",
              "House                                    \n",
              "0                                0.098985\n",
              "Dothraki                        -0.108385\n",
              "Free Folk                        0.064064\n",
              "House Baelish                    0.126756\n",
              "House Baratheon                 -0.064415\n",
              "House Bronn                     -0.003824\n",
              "House Cassel                    -0.127362\n",
              "House Clegane                   -0.095792\n",
              "House Greyjoy                   -0.038064\n",
              "House Lannister                  0.046368\n",
              "House Mormont                    0.079796\n",
              "House Stark                      0.011350\n",
              "House Targaryen                  0.035417\n",
              "House Tarly                      0.019794\n",
              "House Trant                     -0.282800\n",
              "House Tyrell                    -0.006087\n",
              "Night's Watch                   -0.115121"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQekypR8oox3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "6d8fa65f-4332-489b-86e1-c707e0713d8a"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "#Plotting distributions \n",
        "House_Baelish=GOT1[GOT1.House=='House Baelish']\n",
        "House_Baratheon=GOT1[GOT1.House=='House Baratheon']\n",
        "House_Clegane=GOT1[GOT1.House=='House Clegane']\n",
        "House_Lannister=GOT1[GOT1.House=='House Lannister']\n",
        "House_Greyjoy=GOT1[GOT1.House=='House Greyjoy']\n",
        "House_Targaryen=GOT1[GOT1.House=='House Targaryen']\n",
        "House_Tyrell=GOT1[GOT1.House=='House Tyrell']\n",
        "House_Stark=GOT1[GOT1.House=='House Stark']\n",
        "\n",
        "#sns.distplot(House_Baelish[['Sentiment_Compound_Score']], hist=False, rug=True,label='Baelish')\n",
        "sns.distplot(House_Baratheon[['Sentiment_Compound_Score']], hist=False, rug=True,label='Baratheon')\n",
        "\n",
        "#sns.distplot(House_Clegane[['Sentiment_Compound_Score']], hist=False, rug=True,label='Clegane')\n",
        "\n",
        "sns.distplot(House_Lannister[['Sentiment_Compound_Score']], hist=False, rug=True, label='Lannister')\n",
        "\n",
        "sns.distplot(House_Greyjoy[['Sentiment_Compound_Score']], hist=False, rug=True,label='Greyjoy')\n",
        "sns.distplot(House_Targaryen[['Sentiment_Compound_Score']], hist=False, rug=True,label='Targaryen')\n",
        "sns.distplot(House_Tyrell[['Sentiment_Compound_Score']], hist=False, rug=True,label='Tyrell')\n",
        "sns.distplot(House_Stark[['Sentiment_Compound_Score']], hist=False, rug=True,label='Stark')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD4CAYAAAATpHZ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqgXRpK6UUi3I/wM7CCM/ICjJIAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQXWm9h-xOTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}