{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GOT Project.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trong-shen/Game-of-Throne-Project/blob/master/GOT_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kvbsdq0-guNm",
        "colab_type": "text"
      },
      "source": [
        "Load the CSV file from Github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt3EaJ_AgV2I",
        "colab_type": "code",
        "outputId": "e3340b88-64e0-4e86-d7ac-4df253451a14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "GOT= pd.read_csv('https://raw.githubusercontent.com/trong-shen/Game-of-Throne-Project/master/Game_of_Thrones_Script_clean.csv')\n",
        "char_info=pd.read_csv('https://raw.githubusercontent.com/trong-shen/Game-of-Throne-Project/master/got_table.csv')\n",
        "print(len(GOT))\n",
        "\n",
        "#Extract only the house data from char_info\n",
        "House=char_info[['name','house']]\n",
        "\n",
        "#Created a function to apply globally to the data frame\n",
        "def return_house(name):\n",
        "  house_dict=dict(zip(House.name,House.house))\n",
        "  try: \n",
        "    house=house_dict[name]\n",
        "    return(house)\n",
        "  except KeyError:\n",
        "    return(float(\"Nan\"))\n",
        "\n",
        "# Apply the house dict function to the whole GOT dataframe\n",
        "GOT['House']=GOT['Name'].apply(lambda x:return_house(x))\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23911\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Release Date</th>\n",
              "      <th>Season</th>\n",
              "      <th>Episode</th>\n",
              "      <th>Episode Title</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>House</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>waymar royce</td>\n",
              "      <td>What do you expect? They're savages. One lot s...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>will</td>\n",
              "      <td>I've never seen wildlings do a thing like this...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>waymar royce</td>\n",
              "      <td>How close did you get?</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>will</td>\n",
              "      <td>Close as any man would.</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>gared</td>\n",
              "      <td>We should head back to the wall.</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>catelyn stark</td>\n",
              "      <td>You can always say no, Ned.</td>\n",
              "      <td>House Stark</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>catelyn stark</td>\n",
              "      <td>We need plenty of candles for Lord Tyrion's ch...</td>\n",
              "      <td>House Stark</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>maester luwin</td>\n",
              "      <td>I'm told he drinks all night.</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>catelyn stark</td>\n",
              "      <td>How much could he possibly drink? A man of his...</td>\n",
              "      <td>House Stark</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>luwin</td>\n",
              "      <td>We've brought up eight barrels of ale from the...</td>\n",
              "      <td>House Stark</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows Ã— 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Release Date  ...        House\n",
              "0     4/17/2011  ...          NaN\n",
              "1     4/17/2011  ...          NaN\n",
              "2     4/17/2011  ...          NaN\n",
              "3     4/17/2011  ...          NaN\n",
              "4     4/17/2011  ...          NaN\n",
              "..          ...  ...          ...\n",
              "95    4/17/2011  ...  House Stark\n",
              "96    4/17/2011  ...  House Stark\n",
              "97    4/17/2011  ...          NaN\n",
              "98    4/17/2011  ...  House Stark\n",
              "99    4/17/2011  ...  House Stark\n",
              "\n",
              "[100 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdrjeoBTukgp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Extract only season one data and two data\n",
        "GOT1=GOT[GOT.Season==\"Season 1\"]\n",
        "GOT2=GOT[GOT.Season==\"Season 2\"]\n",
        "\n",
        "print(GOT1.head())\n",
        "print(GOT1.info())\n",
        "\n",
        "print(GOT2.head())\n",
        "print(GOT1.info())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbaevQcMf3Qj",
        "colab_type": "code",
        "outputId": "365de656-ab09-435c-a4a2-10373956cec7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "# Keep character lines if and only if they still exist in season 2\n",
        "\n",
        "#Find a unique list of characters in season 2\n",
        "\n",
        "char_S2=GOT2.Name.unique()\n",
        "print(len(char_S2))\n",
        "\n",
        "#Filter S1 data if characters are in S2\n",
        "GOT1_modified=GOT1[GOT1['Name'].isin(char_S2)]\n",
        "print(len(GOT1_modified.Name.unique()))\n",
        "print(GOT1_modified.Name.unique())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "136\n",
            "49\n",
            "['jon snow' 'sansa stark' 'robb stark' 'catelyn stark' 'bran stark'\n",
            " 'theon greyjoy' 'jaime lannister' 'cersei lannister' 'luwin' 'arya stark'\n",
            " 'tyrion lannister' 'ros' 'daenerys targaryen' 'jorah mormont'\n",
            " 'khal drogo' 'sandor clegane' 'doreah' 'irri' 'joffrey lannister'\n",
            " 'myrcella baratheon' 'rodrick cassel' 'soldier' 'varys' 'renly baratheon'\n",
            " 'petyr baelish' 'grand maester pycelle' 'guard' 'jeor mormont' 'grenn'\n",
            " 'lancel lannister' 'rakharo' 'yoren' 'sam tarly' 'janos'\n",
            " 'gendry baratheon' 'bronn' 'loras tyrell' 'osha' 'wildling' 'man'\n",
            " 'tywin lannister' 'meryn trant' 'kevan lannister' 'all' 'prostitute'\n",
            " 'shae' 'rickon stark' 'karstark' 'hot pie']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUhNemrdcXNg",
        "colab_type": "code",
        "outputId": "68c1bbd4-c75f-448e-c389-555d6445e19c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "source": [
        "\n",
        "#Further filter based on characters of interest \n",
        "\n",
        "important_names=np.delete(GOT1_modified.Name.unique(),21);\n",
        "important_names=np.delete(important_names,25);\n",
        "print(important_names)\n",
        "\n",
        "\n",
        "print(len(important_names))\n",
        "\n",
        "GOT1_modified=GOT1_modified[GOT1_modified['Name'].isin(important_names)]\n",
        "GOT1_modified.head()\n",
        "print(len(GOT1_modified.Name.unique()))\n",
        "\n",
        "GOT1_modified.info()\n",
        "GOT1_=GOT1_modified"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['jon snow' 'sansa stark' 'robb stark' 'catelyn stark' 'bran stark'\n",
            " 'theon greyjoy' 'jaime lannister' 'cersei lannister' 'luwin' 'arya stark'\n",
            " 'tyrion lannister' 'ros' 'daenerys targaryen' 'jorah mormont'\n",
            " 'khal drogo' 'sandor clegane' 'doreah' 'irri' 'joffrey lannister'\n",
            " 'myrcella baratheon' 'rodrick cassel' 'varys' 'renly baratheon'\n",
            " 'petyr baelish' 'grand maester pycelle' 'jeor mormont' 'grenn'\n",
            " 'lancel lannister' 'rakharo' 'yoren' 'sam tarly' 'janos'\n",
            " 'gendry baratheon' 'bronn' 'loras tyrell' 'osha' 'wildling' 'man'\n",
            " 'tywin lannister' 'meryn trant' 'kevan lannister' 'all' 'prostitute'\n",
            " 'shae' 'rickon stark' 'karstark' 'hot pie']\n",
            "47\n",
            "47\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 2134 entries, 15 to 3178\n",
            "Data columns (total 6 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   Release Date   2134 non-null   object\n",
            " 1   Season         2134 non-null   object\n",
            " 2   Episode        2134 non-null   object\n",
            " 3   Episode Title  2134 non-null   object\n",
            " 4   Name           2134 non-null   object\n",
            " 5   Sentence       2134 non-null   object\n",
            "dtypes: object(6)\n",
            "memory usage: 116.7+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRuqNksehKo7",
        "colab_type": "code",
        "outputId": "b207490c-0fdd-4a9b-9a4a-5dc1eaa2e982",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('all')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOo0mZPmOaqr",
        "colab_type": "code",
        "outputId": "b5944e91-c1ab-4e59-de32-6405786a8837",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "#Tokenize the words and remove between words punctunations\n",
        "tokenizer=nltk.RegexpTokenizer(r\"\\w+\")\n",
        "GOT1['Tokenized_Sentence']=GOT1.Sentence.apply(lambda x:tokenizer.tokenize(x.lower()))\n",
        "GOT1.head()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Release Date</th>\n",
              "      <th>Season</th>\n",
              "      <th>Episode</th>\n",
              "      <th>Episode Title</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Tokenized_Sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>waymar royce</td>\n",
              "      <td>What do you expect? They're savages. One lot s...</td>\n",
              "      <td>[what, do, you, expect, they, re, savages, one...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>will</td>\n",
              "      <td>I've never seen wildlings do a thing like this...</td>\n",
              "      <td>[i, ve, never, seen, wildlings, do, a, thing, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>waymar royce</td>\n",
              "      <td>How close did you get?</td>\n",
              "      <td>[how, close, did, you, get]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>will</td>\n",
              "      <td>Close as any man would.</td>\n",
              "      <td>[close, as, any, man, would]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>gared</td>\n",
              "      <td>We should head back to the wall.</td>\n",
              "      <td>[we, should, head, back, to, the, wall]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Release Date  ...                                 Tokenized_Sentence\n",
              "0    4/17/2011  ...  [what, do, you, expect, they, re, savages, one...\n",
              "1    4/17/2011  ...  [i, ve, never, seen, wildlings, do, a, thing, ...\n",
              "2    4/17/2011  ...                        [how, close, did, you, get]\n",
              "3    4/17/2011  ...                       [close, as, any, man, would]\n",
              "4    4/17/2011  ...            [we, should, head, back, to, the, wall]\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyBF3RVqPFf1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Remove stopwords for sentiment analysis\n",
        "stopword=nltk.corpus.stopwords.words('english')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J8WRWnGeFvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define a function to remove stop words\n",
        "def remove_stopwords(tokenized_sentence):\n",
        "  text=[word for word in tokenized_sentence if word not in stopword]\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGBoLfGGefi_",
        "colab_type": "code",
        "outputId": "c2198f29-e8ec-45f7-fda9-53ed0c0f6fa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "#Implement the stop words function to a new column \n",
        "GOT1['Tokenized_No_Stop']=GOT1.Tokenized_Sentence.apply(lambda x:remove_stopwords(x))\n",
        "GOT1.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Release Date</th>\n",
              "      <th>Season</th>\n",
              "      <th>Episode</th>\n",
              "      <th>Episode Title</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Tokenized_Sentence</th>\n",
              "      <th>Tokenized_No_Stop</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>waymar royce</td>\n",
              "      <td>What do you expect? They're savages. One lot s...</td>\n",
              "      <td>[what, do, you, expect, they, re, savages, one...</td>\n",
              "      <td>[expect, savages, one, lot, steals, goat, anot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>will</td>\n",
              "      <td>I've never seen wildlings do a thing like this...</td>\n",
              "      <td>[i, ve, never, seen, wildlings, do, a, thing, ...</td>\n",
              "      <td>[never, seen, wildlings, thing, like, never, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>waymar royce</td>\n",
              "      <td>How close did you get?</td>\n",
              "      <td>[how, close, did, you, get]</td>\n",
              "      <td>[close, get]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>will</td>\n",
              "      <td>Close as any man would.</td>\n",
              "      <td>[close, as, any, man, would]</td>\n",
              "      <td>[close, man, would]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>gared</td>\n",
              "      <td>We should head back to the wall.</td>\n",
              "      <td>[we, should, head, back, to, the, wall]</td>\n",
              "      <td>[head, back, wall]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Release Date  ...                                  Tokenized_No_Stop\n",
              "0    4/17/2011  ...  [expect, savages, one, lot, steals, goat, anot...\n",
              "1    4/17/2011  ...  [never, seen, wildlings, thing, like, never, s...\n",
              "2    4/17/2011  ...                                       [close, get]\n",
              "3    4/17/2011  ...                                [close, man, would]\n",
              "4    4/17/2011  ...                                 [head, back, wall]\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnF0bKWXe63D",
        "colab_type": "code",
        "outputId": "b694cb74-93b3-4703-ebd3-96fcd140e4f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "# Stemming of the Non Stop Words Column\n",
        "wn=nltk.WordNetLemmatizer()\n",
        "\n",
        "def stem_reduction(tokenized_sentence):\n",
        "  sentence=[wn.lemmatize(word) for word in tokenized_sentence]\n",
        "  return (sentence)\n",
        "\n",
        "GOT1['Stemmed_Sentence']=GOT1['Tokenized_No_Stop'].apply(lambda x:stem_reduction(x))\n",
        "GOT1.tail()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Release Date</th>\n",
              "      <th>Season</th>\n",
              "      <th>Episode</th>\n",
              "      <th>Episode Title</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Tokenized_Sentence</th>\n",
              "      <th>Tokenized_No_Stop</th>\n",
              "      <th>Stemmed_Sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3174</th>\n",
              "      <td>6/19/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 10</td>\n",
              "      <td>Fire and Blood</td>\n",
              "      <td>daenerys targaryen</td>\n",
              "      <td>ls that what you fear? You will be my khalasar...</td>\n",
              "      <td>[ls, that, what, you, fear, you, will, be, my,...</td>\n",
              "      <td>[ls, fear, khalasar, l, see, faces, slaves, l,...</td>\n",
              "      <td>[l, fear, khalasar, l, see, face, slave, l, fr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3175</th>\n",
              "      <td>6/19/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 10</td>\n",
              "      <td>Fire and Blood</td>\n",
              "      <td>daenerys targaryen</td>\n",
              "      <td>Ser Jorah, bind this woman to the pyre. You sw...</td>\n",
              "      <td>[ser, jorah, bind, this, woman, to, the, pyre,...</td>\n",
              "      <td>[ser, jorah, bind, woman, pyre, swore, obey]</td>\n",
              "      <td>[ser, jorah, bind, woman, pyre, swore, obey]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3176</th>\n",
              "      <td>6/19/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 10</td>\n",
              "      <td>Fire and Blood</td>\n",
              "      <td>mirri maz duur</td>\n",
              "      <td>You will not hear me scream.</td>\n",
              "      <td>[you, will, not, hear, me, scream]</td>\n",
              "      <td>[hear, scream]</td>\n",
              "      <td>[hear, scream]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3177</th>\n",
              "      <td>6/19/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 10</td>\n",
              "      <td>Fire and Blood</td>\n",
              "      <td>daenerys targaryen</td>\n",
              "      <td>I will. But it is not your screams I want. Onl...</td>\n",
              "      <td>[i, will, but, it, is, not, your, screams, i, ...</td>\n",
              "      <td>[screams, want, life]</td>\n",
              "      <td>[scream, want, life]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3178</th>\n",
              "      <td>6/19/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 10</td>\n",
              "      <td>Fire and Blood</td>\n",
              "      <td>jorah mormont</td>\n",
              "      <td>Blood of my blood.</td>\n",
              "      <td>[blood, of, my, blood]</td>\n",
              "      <td>[blood, blood]</td>\n",
              "      <td>[blood, blood]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Release Date  ...                                   Stemmed_Sentence\n",
              "3174    6/19/2011  ...  [l, fear, khalasar, l, see, face, slave, l, fr...\n",
              "3175    6/19/2011  ...       [ser, jorah, bind, woman, pyre, swore, obey]\n",
              "3176    6/19/2011  ...                                     [hear, scream]\n",
              "3177    6/19/2011  ...                               [scream, want, life]\n",
              "3178    6/19/2011  ...                                     [blood, blood]\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzKLVQ4f9bRK",
        "colab_type": "code",
        "outputId": "eda3da63-2f1c-44ae-e523-6e7b7e4ad0c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "#Join the Tokenized Stemmed_Sentence into a string of sentence\n",
        "\n",
        "def convert_sentence (tokenized_sentence):\n",
        "  sentence=''\n",
        "  for word in tokenized_sentence:\n",
        "    sentence=sentence+\" \"+word\n",
        "  return sentence\n",
        "\n",
        "GOT1['Stemmed_Sentence_Non_Token']=GOT1['Stemmed_Sentence'].apply(lambda x:convert_sentence(x))\n",
        "\n",
        "GOT1['Stemmed_Sentence_Non_Token'].head()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     expect savage one lot steal goat another lot ...\n",
              "1     never seen wildlings thing like never seen th...\n",
              "2                                            close get\n",
              "3                                      close man would\n",
              "4                                       head back wall\n",
              "Name: Stemmed_Sentence_Non_Token, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek4IbNjs6ob5",
        "colab_type": "code",
        "outputId": "223d9bbe-4be4-48b0-af3b-ef5b391494d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "#Use Vader Sentiment analysis tool \n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sid=SentimentIntensityAnalyzer()\n",
        "\n",
        "GOT1['Sentiment']=GOT1['Stemmed_Sentence_Non_Token'].apply(lambda x:sid.polarity_scores(x))\n",
        "\n",
        "print(GOT1.Sentence[150])\n",
        "print(GOT1.Stemmed_Sentence[150])\n",
        "print(GOT1.Sentiment[150])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The queen has two brothers?\n",
            "['queen', 'two', 'brother']\n",
            "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}