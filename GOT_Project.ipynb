{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GOT Project.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trong-shen/Game-of-Throne-Project/blob/master/GOT_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kvbsdq0-guNm",
        "colab_type": "text"
      },
      "source": [
        "Load the CSV file from Github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt3EaJ_AgV2I",
        "colab_type": "code",
        "outputId": "6ac5676d-ff03-4a01-f8a0-caca213774f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "\n",
        "\n",
        "GOT= pd.read_csv('https://raw.githubusercontent.com/trong-shen/Game-of-Throne-Project/master/Game_of_Thrones_Script_clean.csv')\n",
        "char_info=pd.read_csv('https://raw.githubusercontent.com/trong-shen/Game-of-Throne-Project/master/got_table.csv')\n",
        "print(len(GOT))\n",
        "\n",
        "#Extract only the house data from char_info\n",
        "House=char_info[['name','house']]\n",
        "\n",
        "#Created a function to apply globally to the data frame\n",
        "def return_house(name):\n",
        "  house_dict=dict(zip(House.name,House.house))\n",
        "  try: \n",
        "    house=house_dict[name]\n",
        "    return(house)\n",
        "  except KeyError:\n",
        "    return(float(\"Nan\"))\n",
        "\n",
        "# Apply the house dict function to the whole GOT dataframe\n",
        "GOT['House']=GOT['Name'].apply(lambda x:return_house(x))\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "23911\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9thUHG5FPCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT79c3JB_d23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#list of contractions and the expanded mapping used for cleaning the data\n",
        "CONTRACTION_MAP = {\n",
        "\"ain't\": \"is not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"I'd\": \"I would\",\n",
        "\"I'd've\": \"I would have\",\n",
        "\"I'll\": \"I will\",\n",
        "\"I'll've\": \"I will have\",\n",
        "\"I'm\": \"I am\",\n",
        "\"I've\": \"I have\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cle0cONgAULs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define a function, expand_contractions, which takes a string and expands all contractions within the string using contraction_map\n",
        "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
        "    expanded = ''\n",
        "    text = text.lower() #make all text lowercase\n",
        "    wordList = text.split() #put text into a list of words\n",
        "    for i in range(len(wordList)):\n",
        "        if wordList[i] in contraction_mapping.keys(): #for each word, if it is a contraction in the listing\n",
        "            expanded = expanded + ' ' + contraction_mapping[wordList[i]] #then replace with the expanded version\n",
        "        else:\n",
        "            expanded = expanded + ' ' + wordList[i] #otherwise, keep the original word\n",
        "    return expanded\n",
        "\n",
        "#define a function, remove_punctuation, which takes in a string and removes all punctuation \n",
        "def remove_punctuation(s):\n",
        "    s = s.translate(str.maketrans('','',string.punctuation)) #take out punctuation in the sentence\n",
        "    j = nltk.word_tokenize(s.lower()) #put each word in the sentence within a list, j\n",
        "    return s\n",
        "\n",
        "#define function, clean_sentences, which removes punctuation and expands all contractions in a sentence\n",
        "def clean_sentences(text):\n",
        "    return remove_punctuation(expand_contractions(text))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOXNk319AXLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Expand contractions, remove punctuation all in one function clean_sentence\n",
        "GOT['Sentences_Clean'] = GOT.Sentence.apply(lambda x:clean_sentences(x))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS53Yh89CAnb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calculate words per line, assuming contractions are all expanded\n",
        "GOT[\"Num_Words\"] = GOT.Sentences_Clean.apply(lambda x: len(x.split()))\n",
        "GOT.to_csv (r'GOT_house_csv.csv', index = False, header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5ln7QIcDK6q",
        "colab_type": "text"
      },
      "source": [
        "# Now we will  prepare data for our Machine Learning Predictive model by only looking at msotly season 1 data and a subsection of season 2 data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdrjeoBTukgp",
        "colab_type": "code",
        "outputId": "7c809b7c-7fdd-47b3-8cce-d08a6f085fbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "#Extract only season one data and two data\n",
        "GOT1=GOT[GOT.Season==\"Season 1\"]\n",
        "GOT2=GOT[GOT.Season==\"Season 2\"]\n",
        "\n",
        "print(GOT1.head())\n",
        "print(GOT1.info())\n",
        "\n",
        "print(GOT2.head())\n",
        "print(GOT1.info())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Release Date  ... Num_Words\n",
            "0    4/17/2011  ...        27\n",
            "1    4/17/2011  ...        23\n",
            "2    4/17/2011  ...         5\n",
            "3    4/17/2011  ...         5\n",
            "4    4/17/2011  ...         7\n",
            "\n",
            "[5 rows x 9 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 3179 entries, 0 to 3178\n",
            "Data columns (total 9 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   Release Date     3179 non-null   object\n",
            " 1   Season           3179 non-null   object\n",
            " 2   Episode          3179 non-null   object\n",
            " 3   Episode Title    3179 non-null   object\n",
            " 4   Name             3179 non-null   object\n",
            " 5   Sentence         3179 non-null   object\n",
            " 6   House            2907 non-null   object\n",
            " 7   Sentences_Clean  3179 non-null   object\n",
            " 8   Num_Words        3179 non-null   int64 \n",
            "dtypes: int64(1), object(8)\n",
            "memory usage: 248.4+ KB\n",
            "None\n",
            "     Release Date    Season  ...                     Sentences_Clean Num_Words\n",
            "3179     4/1/2012  Season 2  ...        well struck… well struck dog         5\n",
            "3180     4/1/2012  Season 2  ...                   did you like that         4\n",
            "3181     4/1/2012  Season 2  ...       it was well struck your grace         6\n",
            "3182     4/1/2012  Season 2  ...   i already said it was well struck         7\n",
            "3183     4/1/2012  Season 2  ...                      yes your grace         3\n",
            "\n",
            "[5 rows x 9 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 3179 entries, 0 to 3178\n",
            "Data columns (total 9 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   Release Date     3179 non-null   object\n",
            " 1   Season           3179 non-null   object\n",
            " 2   Episode          3179 non-null   object\n",
            " 3   Episode Title    3179 non-null   object\n",
            " 4   Name             3179 non-null   object\n",
            " 5   Sentence         3179 non-null   object\n",
            " 6   House            2907 non-null   object\n",
            " 7   Sentences_Clean  3179 non-null   object\n",
            " 8   Num_Words        3179 non-null   int64 \n",
            "dtypes: int64(1), object(8)\n",
            "memory usage: 248.4+ KB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbaevQcMf3Qj",
        "colab_type": "code",
        "outputId": "13523fc1-a5f8-4eb8-820e-59859bb646c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Keep character lines if and only if they still exist in season 2\n",
        "\n",
        "#Find a unique list of characters in season 2\n",
        "\n",
        "char_S2=GOT2.Name.unique()\n",
        "print(len(char_S2))\n",
        "\n",
        "#Filter S1 data if characters are in S2\n",
        "GOT1_modified=GOT1[GOT1['Name'].isin(char_S2)]\n",
        "print(len(GOT1_modified.Name.unique()))\n",
        "print(GOT1_modified.Name.unique())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "136\n",
            "49\n",
            "['jon snow' 'sansa stark' 'robb stark' 'catelyn stark' 'bran stark'\n",
            " 'theon greyjoy' 'jaime lannister' 'cersei lannister' 'luwin' 'arya stark'\n",
            " 'tyrion lannister' 'ros' 'daenerys targaryen' 'jorah mormont'\n",
            " 'khal drogo' 'sandor clegane' 'doreah' 'irri' 'joffrey lannister'\n",
            " 'myrcella baratheon' 'rodrick cassel' 'soldier' 'varys' 'renly baratheon'\n",
            " 'petyr baelish' 'grand maester pycelle' 'guard' 'jeor mormont' 'grenn'\n",
            " 'lancel lannister' 'rakharo' 'yoren' 'sam tarly' 'janos'\n",
            " 'gendry baratheon' 'bronn' 'loras tyrell' 'osha' 'wildling' 'man'\n",
            " 'tywin lannister' 'meryn trant' 'kevan lannister' 'all' 'prostitute'\n",
            " 'shae' 'rickon stark' 'karstark' 'hot pie']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUhNemrdcXNg",
        "colab_type": "code",
        "outputId": "b24679d3-9fda-48de-843a-dd3a2365b06f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "#Further filter based on characters of interest \n",
        "#removing generic characters such as solider, guard, prostitute, etc\n",
        "important_names=np.delete(GOT1_modified.Name.unique(),(21,26,38,39,43,44));\n",
        "print(important_names)\n",
        "\n",
        "\n",
        "print(len(important_names))\n",
        "\n",
        "GOT1_modified=GOT1_modified[GOT1_modified['Name'].isin(important_names)]\n",
        "GOT1_modified.head()\n",
        "print(len(GOT1_modified.Name.unique()))\n",
        "\n",
        "GOT1_modified.info()\n",
        "GOT1=GOT1_modified\n",
        "GOT1.info()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['jon snow' 'sansa stark' 'robb stark' 'catelyn stark' 'bran stark'\n",
            " 'theon greyjoy' 'jaime lannister' 'cersei lannister' 'luwin' 'arya stark'\n",
            " 'tyrion lannister' 'ros' 'daenerys targaryen' 'jorah mormont'\n",
            " 'khal drogo' 'sandor clegane' 'doreah' 'irri' 'joffrey lannister'\n",
            " 'myrcella baratheon' 'rodrick cassel' 'varys' 'renly baratheon'\n",
            " 'petyr baelish' 'grand maester pycelle' 'jeor mormont' 'grenn'\n",
            " 'lancel lannister' 'rakharo' 'yoren' 'sam tarly' 'janos'\n",
            " 'gendry baratheon' 'bronn' 'loras tyrell' 'osha' 'tywin lannister'\n",
            " 'meryn trant' 'kevan lannister' 'shae' 'rickon stark' 'karstark'\n",
            " 'hot pie']\n",
            "43\n",
            "43\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 2120 entries, 15 to 3178\n",
            "Data columns (total 9 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   Release Date     2120 non-null   object\n",
            " 1   Season           2120 non-null   object\n",
            " 2   Episode          2120 non-null   object\n",
            " 3   Episode Title    2120 non-null   object\n",
            " 4   Name             2120 non-null   object\n",
            " 5   Sentence         2120 non-null   object\n",
            " 6   House            2081 non-null   object\n",
            " 7   Sentences_Clean  2120 non-null   object\n",
            " 8   Num_Words        2120 non-null   int64 \n",
            "dtypes: int64(1), object(8)\n",
            "memory usage: 165.6+ KB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 2120 entries, 15 to 3178\n",
            "Data columns (total 9 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   Release Date     2120 non-null   object\n",
            " 1   Season           2120 non-null   object\n",
            " 2   Episode          2120 non-null   object\n",
            " 3   Episode Title    2120 non-null   object\n",
            " 4   Name             2120 non-null   object\n",
            " 5   Sentence         2120 non-null   object\n",
            " 6   House            2081 non-null   object\n",
            " 7   Sentences_Clean  2120 non-null   object\n",
            " 8   Num_Words        2120 non-null   int64 \n",
            "dtypes: int64(1), object(8)\n",
            "memory usage: 165.6+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOo0mZPmOaqr",
        "colab_type": "code",
        "outputId": "36010876-99ed-48ca-f535-a82880d9d855",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 864
        }
      },
      "source": [
        "#Tokenize the words and remove between words punctunations\n",
        "tokenizer=nltk.RegexpTokenizer(r\"\\w+\")\n",
        "GOT1['Tokenized_Sentence']=GOT1.Sentences_Clean.apply(lambda x:tokenizer.tokenize(x.lower()))\n",
        "GOT1.head(100)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Release Date</th>\n",
              "      <th>Season</th>\n",
              "      <th>Episode</th>\n",
              "      <th>Episode Title</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>House</th>\n",
              "      <th>Sentences_Clean</th>\n",
              "      <th>Num_Words</th>\n",
              "      <th>Tokenized_Sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>Go on. Father's watching.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>go on fathers watching</td>\n",
              "      <td>4</td>\n",
              "      <td>[go, on, fathers, watching]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>And your mother.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>and your mother</td>\n",
              "      <td>3</td>\n",
              "      <td>[and, your, mother]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>sansa stark</td>\n",
              "      <td>Thank you.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>thank you</td>\n",
              "      <td>2</td>\n",
              "      <td>[thank, you]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>Don't think too much, Bran.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>do not think too much bran</td>\n",
              "      <td>6</td>\n",
              "      <td>[do, not, think, too, much, bran]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>robb stark</td>\n",
              "      <td>Relax your bow arm.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>relax your bow arm</td>\n",
              "      <td>4</td>\n",
              "      <td>[relax, your, bow, arm]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>tyrion lannister</td>\n",
              "      <td>She has odd cravings, our sister.</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>she has odd cravings our sister</td>\n",
              "      <td>6</td>\n",
              "      <td>[she, has, odd, cravings, our, sister]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jaime lannister</td>\n",
              "      <td>A family trait. Now, the Starks are feasting u...</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>a family trait now the starks are feasting us...</td>\n",
              "      <td>19</td>\n",
              "      <td>[a, family, trait, now, the, starks, are, feas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>tyrion lannister</td>\n",
              "      <td>I'm sorry, I've begun the feast a bit early. A...</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>i am sorry i have begun the feast a bit early...</td>\n",
              "      <td>19</td>\n",
              "      <td>[i, am, sorry, i, have, begun, the, feast, a, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jaime lannister</td>\n",
              "      <td>I thought you might say that. But since we're ...</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>i thought you might say that but since we are...</td>\n",
              "      <td>20</td>\n",
              "      <td>[i, thought, you, might, say, that, but, since...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>tyrion lannister</td>\n",
              "      <td>Close the door!</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>close the door</td>\n",
              "      <td>3</td>\n",
              "      <td>[close, the, door]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Release Date  ...                                 Tokenized_Sentence\n",
              "15     4/17/2011  ...                        [go, on, fathers, watching]\n",
              "16     4/17/2011  ...                                [and, your, mother]\n",
              "18     4/17/2011  ...                                       [thank, you]\n",
              "21     4/17/2011  ...                  [do, not, think, too, much, bran]\n",
              "22     4/17/2011  ...                            [relax, your, bow, arm]\n",
              "..           ...  ...                                                ...\n",
              "163    4/17/2011  ...             [she, has, odd, cravings, our, sister]\n",
              "164    4/17/2011  ...  [a, family, trait, now, the, starks, are, feas...\n",
              "165    4/17/2011  ...  [i, am, sorry, i, have, begun, the, feast, a, ...\n",
              "166    4/17/2011  ...  [i, thought, you, might, say, that, but, since...\n",
              "167    4/17/2011  ...                                 [close, the, door]\n",
              "\n",
              "[100 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyBF3RVqPFf1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Remove stopwords for sentiment analysis\n",
        "stopword=nltk.corpus.stopwords.words('english')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J8WRWnGeFvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define a function to remove stop words\n",
        "def remove_stopwords(tokenized_sentence):\n",
        "  text=[word for word in tokenized_sentence if word not in stopword]\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGBoLfGGefi_",
        "colab_type": "code",
        "outputId": "492a0217-9309-4d46-a3b4-ad9b469bdcae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "#Implement the stop words function to a new column \n",
        "GOT1['Tokenized_No_Stop']=GOT1.Tokenized_Sentence.apply(lambda x:remove_stopwords(x))\n",
        "GOT1.head()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Release Date</th>\n",
              "      <th>Season</th>\n",
              "      <th>Episode</th>\n",
              "      <th>Episode Title</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>House</th>\n",
              "      <th>Sentences_Clean</th>\n",
              "      <th>Num_Words</th>\n",
              "      <th>Tokenized_Sentence</th>\n",
              "      <th>Tokenized_No_Stop</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>Go on. Father's watching.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>go on fathers watching</td>\n",
              "      <td>4</td>\n",
              "      <td>[go, on, fathers, watching]</td>\n",
              "      <td>[go, fathers, watching]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>And your mother.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>and your mother</td>\n",
              "      <td>3</td>\n",
              "      <td>[and, your, mother]</td>\n",
              "      <td>[mother]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>sansa stark</td>\n",
              "      <td>Thank you.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>thank you</td>\n",
              "      <td>2</td>\n",
              "      <td>[thank, you]</td>\n",
              "      <td>[thank]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>Don't think too much, Bran.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>do not think too much bran</td>\n",
              "      <td>6</td>\n",
              "      <td>[do, not, think, too, much, bran]</td>\n",
              "      <td>[think, much, bran]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>robb stark</td>\n",
              "      <td>Relax your bow arm.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>relax your bow arm</td>\n",
              "      <td>4</td>\n",
              "      <td>[relax, your, bow, arm]</td>\n",
              "      <td>[relax, bow, arm]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Release Date  ...        Tokenized_No_Stop\n",
              "15    4/17/2011  ...  [go, fathers, watching]\n",
              "16    4/17/2011  ...                 [mother]\n",
              "18    4/17/2011  ...                  [thank]\n",
              "21    4/17/2011  ...      [think, much, bran]\n",
              "22    4/17/2011  ...        [relax, bow, arm]\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnF0bKWXe63D",
        "colab_type": "code",
        "outputId": "8995f24d-a6c7-4908-b14e-84d5c2364649",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "# Stemming of the Non Stop Words Column\n",
        "wn=nltk.WordNetLemmatizer()\n",
        "\n",
        "def stem_reduction(tokenized_sentence):\n",
        "  sentence=[wn.lemmatize(word) for word in tokenized_sentence]\n",
        "  return (sentence)\n",
        "\n",
        "GOT1['Stemmed_Sentence']=GOT1['Tokenized_No_Stop'].apply(lambda x:stem_reduction(x))\n",
        "GOT1.tail()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Release Date</th>\n",
              "      <th>Season</th>\n",
              "      <th>Episode</th>\n",
              "      <th>Episode Title</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>House</th>\n",
              "      <th>Sentences_Clean</th>\n",
              "      <th>Num_Words</th>\n",
              "      <th>Tokenized_Sentence</th>\n",
              "      <th>Tokenized_No_Stop</th>\n",
              "      <th>Stemmed_Sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3173</th>\n",
              "      <td>6/19/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 10</td>\n",
              "      <td>Fire and Blood</td>\n",
              "      <td>jorah mormont</td>\n",
              "      <td>Don't ask me to stand aside as you climb on th...</td>\n",
              "      <td>House Mormont</td>\n",
              "      <td>do not ask me to stand aside as you climb on ...</td>\n",
              "      <td>19</td>\n",
              "      <td>[do, not, ask, me, to, stand, aside, as, you, ...</td>\n",
              "      <td>[ask, stand, aside, climb, pyre, l, watch, burn]</td>\n",
              "      <td>[ask, stand, aside, climb, pyre, l, watch, burn]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3174</th>\n",
              "      <td>6/19/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 10</td>\n",
              "      <td>Fire and Blood</td>\n",
              "      <td>daenerys targaryen</td>\n",
              "      <td>ls that what you fear? You will be my khalasar...</td>\n",
              "      <td>House Targaryen</td>\n",
              "      <td>ls that what you fear you will be my khalasar...</td>\n",
              "      <td>47</td>\n",
              "      <td>[ls, that, what, you, fear, you, will, be, my,...</td>\n",
              "      <td>[ls, fear, khalasar, l, see, faces, slaves, l,...</td>\n",
              "      <td>[l, fear, khalasar, l, see, face, slave, l, fr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3175</th>\n",
              "      <td>6/19/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 10</td>\n",
              "      <td>Fire and Blood</td>\n",
              "      <td>daenerys targaryen</td>\n",
              "      <td>Ser Jorah, bind this woman to the pyre. You sw...</td>\n",
              "      <td>House Targaryen</td>\n",
              "      <td>ser jorah bind this woman to the pyre you swo...</td>\n",
              "      <td>13</td>\n",
              "      <td>[ser, jorah, bind, this, woman, to, the, pyre,...</td>\n",
              "      <td>[ser, jorah, bind, woman, pyre, swore, obey]</td>\n",
              "      <td>[ser, jorah, bind, woman, pyre, swore, obey]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3177</th>\n",
              "      <td>6/19/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 10</td>\n",
              "      <td>Fire and Blood</td>\n",
              "      <td>daenerys targaryen</td>\n",
              "      <td>I will. But it is not your screams I want. Onl...</td>\n",
              "      <td>House Targaryen</td>\n",
              "      <td>i will but it is not your screams i want only...</td>\n",
              "      <td>13</td>\n",
              "      <td>[i, will, but, it, is, not, your, screams, i, ...</td>\n",
              "      <td>[screams, want, life]</td>\n",
              "      <td>[scream, want, life]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3178</th>\n",
              "      <td>6/19/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 10</td>\n",
              "      <td>Fire and Blood</td>\n",
              "      <td>jorah mormont</td>\n",
              "      <td>Blood of my blood.</td>\n",
              "      <td>House Mormont</td>\n",
              "      <td>blood of my blood</td>\n",
              "      <td>4</td>\n",
              "      <td>[blood, of, my, blood]</td>\n",
              "      <td>[blood, blood]</td>\n",
              "      <td>[blood, blood]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Release Date  ...                                   Stemmed_Sentence\n",
              "3173    6/19/2011  ...   [ask, stand, aside, climb, pyre, l, watch, burn]\n",
              "3174    6/19/2011  ...  [l, fear, khalasar, l, see, face, slave, l, fr...\n",
              "3175    6/19/2011  ...       [ser, jorah, bind, woman, pyre, swore, obey]\n",
              "3177    6/19/2011  ...                               [scream, want, life]\n",
              "3178    6/19/2011  ...                                     [blood, blood]\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzKLVQ4f9bRK",
        "colab_type": "code",
        "outputId": "45c0a5bb-1546-4089-a7f0-f2008beae2e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#Join the Tokenized Stemmed_Sentence into a string of sentence for the VADER sentiment analysis\n",
        "\n",
        "def convert_sentence (tokenized_sentence):\n",
        "  sentence=''\n",
        "  for word in tokenized_sentence:\n",
        "    sentence=sentence+\" \"+word\n",
        "  return sentence\n",
        "\n",
        "GOT1['Stemmed_Sentence_Non_Token']=GOT1['Stemmed_Sentence'].apply(lambda x:convert_sentence(x))\n",
        "\n",
        "GOT1['Stemmed_Sentence_Non_Token'].head()\n",
        "\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15     go father watching\n",
              "16                 mother\n",
              "18                  thank\n",
              "21        think much bran\n",
              "22          relax bow arm\n",
              "Name: Stemmed_Sentence_Non_Token, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek4IbNjs6ob5",
        "colab_type": "code",
        "outputId": "58ce9f70-4e22-4deb-829a-a16522afdf9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Use Vader Sentiment analysis tool \n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sid=SentimentIntensityAnalyzer()\n",
        "\n",
        "GOT1['Sentiment']=GOT1['Stemmed_Sentence_Non_Token'].apply(lambda x:sid.polarity_scores(x))\n",
        "\n",
        "def compound_score(Sentiment):\n",
        "  return(Sentiment['compound'])\n",
        "\n",
        "GOT1['Sentiment_Compound_Score']=GOT1['Sentiment'].apply(lambda x:compound_score(x))\n",
        "GOT1.head(100)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Release Date</th>\n",
              "      <th>Season</th>\n",
              "      <th>Episode</th>\n",
              "      <th>Episode Title</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>House</th>\n",
              "      <th>Sentences_Clean</th>\n",
              "      <th>Num_Words</th>\n",
              "      <th>Tokenized_Sentence</th>\n",
              "      <th>Tokenized_No_Stop</th>\n",
              "      <th>Stemmed_Sentence</th>\n",
              "      <th>Stemmed_Sentence_Non_Token</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Sentiment_Compound_Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>Go on. Father's watching.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>go on fathers watching</td>\n",
              "      <td>4</td>\n",
              "      <td>[go, on, fathers, watching]</td>\n",
              "      <td>[go, fathers, watching]</td>\n",
              "      <td>[go, father, watching]</td>\n",
              "      <td>go father watching</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>And your mother.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>and your mother</td>\n",
              "      <td>3</td>\n",
              "      <td>[and, your, mother]</td>\n",
              "      <td>[mother]</td>\n",
              "      <td>[mother]</td>\n",
              "      <td>mother</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>sansa stark</td>\n",
              "      <td>Thank you.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>thank you</td>\n",
              "      <td>2</td>\n",
              "      <td>[thank, you]</td>\n",
              "      <td>[thank]</td>\n",
              "      <td>[thank]</td>\n",
              "      <td>thank</td>\n",
              "      <td>{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound...</td>\n",
              "      <td>0.3612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jon snow</td>\n",
              "      <td>Don't think too much, Bran.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>do not think too much bran</td>\n",
              "      <td>6</td>\n",
              "      <td>[do, not, think, too, much, bran]</td>\n",
              "      <td>[think, much, bran]</td>\n",
              "      <td>[think, much, bran]</td>\n",
              "      <td>think much bran</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>robb stark</td>\n",
              "      <td>Relax your bow arm.</td>\n",
              "      <td>House Stark</td>\n",
              "      <td>relax your bow arm</td>\n",
              "      <td>4</td>\n",
              "      <td>[relax, your, bow, arm]</td>\n",
              "      <td>[relax, bow, arm]</td>\n",
              "      <td>[relax, bow, arm]</td>\n",
              "      <td>relax bow arm</td>\n",
              "      <td>{'neg': 0.0, 'neu': 0.408, 'pos': 0.592, 'comp...</td>\n",
              "      <td>0.4404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>tyrion lannister</td>\n",
              "      <td>She has odd cravings, our sister.</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>she has odd cravings our sister</td>\n",
              "      <td>6</td>\n",
              "      <td>[she, has, odd, cravings, our, sister]</td>\n",
              "      <td>[odd, cravings, sister]</td>\n",
              "      <td>[odd, craving, sister]</td>\n",
              "      <td>odd craving sister</td>\n",
              "      <td>{'neg': 0.535, 'neu': 0.465, 'pos': 0.0, 'comp...</td>\n",
              "      <td>-0.3182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jaime lannister</td>\n",
              "      <td>A family trait. Now, the Starks are feasting u...</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>a family trait now the starks are feasting us...</td>\n",
              "      <td>19</td>\n",
              "      <td>[a, family, trait, now, the, starks, are, feas...</td>\n",
              "      <td>[family, trait, starks, feasting, us, sundown,...</td>\n",
              "      <td>[family, trait, starks, feasting, u, sundown, ...</td>\n",
              "      <td>family trait starks feasting u sundown leave ...</td>\n",
              "      <td>{'neg': 0.348, 'neu': 0.652, 'pos': 0.0, 'comp...</td>\n",
              "      <td>-0.2960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>tyrion lannister</td>\n",
              "      <td>I'm sorry, I've begun the feast a bit early. A...</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>i am sorry i have begun the feast a bit early...</td>\n",
              "      <td>19</td>\n",
              "      <td>[i, am, sorry, i, have, begun, the, feast, a, ...</td>\n",
              "      <td>[sorry, begun, feast, bit, early, first, many,...</td>\n",
              "      <td>[sorry, begun, feast, bit, early, first, many,...</td>\n",
              "      <td>sorry begun feast bit early first many course</td>\n",
              "      <td>{'neg': 0.157, 'neu': 0.843, 'pos': 0.0, 'comp...</td>\n",
              "      <td>-0.0772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>jaime lannister</td>\n",
              "      <td>I thought you might say that. But since we're ...</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>i thought you might say that but since we are...</td>\n",
              "      <td>20</td>\n",
              "      <td>[i, thought, you, might, say, that, but, since...</td>\n",
              "      <td>[thought, might, say, since, short, time, come...</td>\n",
              "      <td>[thought, might, say, since, short, time, come...</td>\n",
              "      <td>thought might say since short time come girl ...</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>4/17/2011</td>\n",
              "      <td>Season 1</td>\n",
              "      <td>Episode 1</td>\n",
              "      <td>Winter is Coming</td>\n",
              "      <td>tyrion lannister</td>\n",
              "      <td>Close the door!</td>\n",
              "      <td>House Lannister</td>\n",
              "      <td>close the door</td>\n",
              "      <td>3</td>\n",
              "      <td>[close, the, door]</td>\n",
              "      <td>[close, door]</td>\n",
              "      <td>[close, door]</td>\n",
              "      <td>close door</td>\n",
              "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Release Date  ... Sentiment_Compound_Score\n",
              "15     4/17/2011  ...                   0.0000\n",
              "16     4/17/2011  ...                   0.0000\n",
              "18     4/17/2011  ...                   0.3612\n",
              "21     4/17/2011  ...                   0.0000\n",
              "22     4/17/2011  ...                   0.4404\n",
              "..           ...  ...                      ...\n",
              "163    4/17/2011  ...                  -0.3182\n",
              "164    4/17/2011  ...                  -0.2960\n",
              "165    4/17/2011  ...                  -0.0772\n",
              "166    4/17/2011  ...                   0.0000\n",
              "167    4/17/2011  ...                   0.0000\n",
              "\n",
              "[100 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yijZWz2Agxx-",
        "colab_type": "code",
        "outputId": "e734717b-ac40-43cb-88e7-ca1ffdd2d06c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "#Set the character and house to category\n",
        "\n",
        "GOT1.info()\n",
        "GOT1['Name'].astype('category')\n",
        "GOT1['House'].astype('category')\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 2120 entries, 15 to 3178\n",
            "Data columns (total 15 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   Release Date                2120 non-null   object \n",
            " 1   Season                      2120 non-null   object \n",
            " 2   Episode                     2120 non-null   object \n",
            " 3   Episode Title               2120 non-null   object \n",
            " 4   Name                        2120 non-null   object \n",
            " 5   Sentence                    2120 non-null   object \n",
            " 6   House                       2081 non-null   object \n",
            " 7   Sentences_Clean             2120 non-null   object \n",
            " 8   Num_Words                   2120 non-null   int64  \n",
            " 9   Tokenized_Sentence          2120 non-null   object \n",
            " 10  Tokenized_No_Stop           2120 non-null   object \n",
            " 11  Stemmed_Sentence            2120 non-null   object \n",
            " 12  Stemmed_Sentence_Non_Token  2120 non-null   object \n",
            " 13  Sentiment                   2120 non-null   object \n",
            " 14  Sentiment_Compound_Score    2120 non-null   float64\n",
            "dtypes: float64(1), int64(1), object(13)\n",
            "memory usage: 265.0+ KB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15          House Stark\n",
              "16          House Stark\n",
              "18          House Stark\n",
              "21          House Stark\n",
              "22          House Stark\n",
              "             ...       \n",
              "3173      House Mormont\n",
              "3174    House Targaryen\n",
              "3175    House Targaryen\n",
              "3177    House Targaryen\n",
              "3178      House Mormont\n",
              "Name: House, Length: 2120, dtype: category\n",
              "Categories (17, object): [0, Dothraki, Free Folk, House Baelish, ..., House Tarly,\n",
              "                          House Trant, House Tyrell, Night's Watch]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aI8nrePfiUr7",
        "colab_type": "code",
        "outputId": "aa8f3409-b36d-495d-ef3b-50229f648290",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        }
      },
      "source": [
        "#A Pivot Table to Show mean Compiund_Score by House\n",
        "table=pd.pivot_table(GOT1,values='Sentiment_Compound_Score',index=['House'],aggfunc=['mean','count'])\n",
        "table"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>mean</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>Sentiment_Compound_Score</th>\n",
              "      <th>Sentiment_Compound_Score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.098985</td>\n",
              "      <td>119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dothraki</th>\n",
              "      <td>-0.108385</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Free Folk</th>\n",
              "      <td>0.064064</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Baelish</th>\n",
              "      <td>0.126756</td>\n",
              "      <td>112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Baratheon</th>\n",
              "      <td>-0.064415</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Bronn</th>\n",
              "      <td>-0.003824</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Cassel</th>\n",
              "      <td>-0.127362</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Clegane</th>\n",
              "      <td>-0.095792</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Greyjoy</th>\n",
              "      <td>-0.038064</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Lannister</th>\n",
              "      <td>0.046368</td>\n",
              "      <td>564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Mormont</th>\n",
              "      <td>0.079796</td>\n",
              "      <td>117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Stark</th>\n",
              "      <td>0.011350</td>\n",
              "      <td>626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Targaryen</th>\n",
              "      <td>0.035417</td>\n",
              "      <td>189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Tarly</th>\n",
              "      <td>0.019794</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Trant</th>\n",
              "      <td>-0.282800</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>House Tyrell</th>\n",
              "      <td>-0.006087</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Night's Watch</th>\n",
              "      <td>-0.115121</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    mean                    count\n",
              "                Sentiment_Compound_Score Sentiment_Compound_Score\n",
              "House                                                            \n",
              "0                               0.098985                      119\n",
              "Dothraki                       -0.108385                       13\n",
              "Free Folk                       0.064064                       25\n",
              "House Baelish                   0.126756                      112\n",
              "House Baratheon                -0.064415                       54\n",
              "House Bronn                    -0.003824                       33\n",
              "House Cassel                   -0.127362                       13\n",
              "House Clegane                  -0.095792                       13\n",
              "House Greyjoy                  -0.038064                       70\n",
              "House Lannister                 0.046368                      564\n",
              "House Mormont                   0.079796                      117\n",
              "House Stark                     0.011350                      626\n",
              "House Targaryen                 0.035417                      189\n",
              "House Tarly                     0.019794                       54\n",
              "House Trant                    -0.282800                        4\n",
              "House Tyrell                   -0.006087                       23\n",
              "Night's Watch                  -0.115121                       52"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQekypR8oox3",
        "colab_type": "code",
        "outputId": "faad4f37-ecbd-48b8-da41-f774b3d72c27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "#Plotting distributions \n",
        "House_Baelish=GOT1[GOT1.House=='House Baelish']\n",
        "House_Baratheon=GOT1[GOT1.House=='House Baratheon']\n",
        "House_Clegane=GOT1[GOT1.House=='House Clegane']\n",
        "House_Lannister=GOT1[GOT1.House=='House Lannister']\n",
        "House_Greyjoy=GOT1[GOT1.House=='House Greyjoy']\n",
        "House_Targaryen=GOT1[GOT1.House=='House Targaryen']\n",
        "House_Tyrell=GOT1[GOT1.House=='House Tyrell']\n",
        "House_Stark=GOT1[GOT1.House=='House Stark']\n",
        "Nights_Watch=GOT1[GOT1.House==\"Night's Watch\"]\n",
        "\n",
        "#sns.distplot(House_Baelish[['Sentiment_Compound_Score']], hist=False, kde=True,label='Baelish')\n",
        "sns.distplot(House_Baratheon[['Sentiment_Compound_Score']], hist=False, kde=True,label='Baratheon')\n",
        "\n",
        "#sns.distplot(House_Clegane[['Sentiment_Compound_Score']], hist=False, kde=True,label='Clegane')\n",
        "\n",
        "#sns.distplot(House_Lannister[['Sentiment_Compound_Score']], hist=False, kde=True, label='Lannister')\n",
        "\n",
        "#sns.distplot(House_Greyjoy[['Sentiment_Compound_Score']], hist=False,kde=True,label='Greyjoy')\n",
        "sns.distplot(House_Targaryen[['Sentiment_Compound_Score']], hist=False,kde=True,label='Targaryen')\n",
        "sns.distplot(House_Tyrell[['Sentiment_Compound_Score']], hist=False, kde=True,label='Tyrell')\n",
        "sns.distplot(House_Stark[['Sentiment_Compound_Score']], hist=False, kde=True,label='Stark')\n",
        "sns.distplot(Nights_Watch[['Sentiment_Compound_Score']], hist=False,kde=True,label='Nights_Watch')\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f3e0c946048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD6CAYAAABebNdxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sU4qnU14SqWo",
        "colab_type": "text"
      },
      "source": [
        "# Applying machine learning models on data set - Naive Bays, Decision Tree, and Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvfbYOlhSpbq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "32a74139-c579-4bf3-a8b3-9156e37a10ef"
      },
      "source": [
        "GOT1['House'].value_counts()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "House Stark        626\n",
              "House Lannister    564\n",
              "House Targaryen    189\n",
              "0                  119\n",
              "House Mormont      117\n",
              "House Baelish      112\n",
              "House Greyjoy       70\n",
              "House Tarly         54\n",
              "House Baratheon     54\n",
              "Night's Watch       52\n",
              "House Bronn         33\n",
              "Free Folk           25\n",
              "House Tyrell        23\n",
              "House Clegane       13\n",
              "Dothraki            13\n",
              "House Cassel        13\n",
              "House Trant          4\n",
              "Name: House, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiKaw5oNXQxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Select a subset that only contains the top four house records in terms of records.\n",
        "house_ml_list=['House Stark','House Lannister','House Targaryen','House Mormont']\n",
        "GOT1_ML=GOT1[GOT1['House'].isin(house_ml_list)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWum89SfXpmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Prepare training and testing dataset\n",
        "NewSentence=GOT1_ML[\"Stemmed_Sentence_Non_Token\"].tolist()\n",
        "#create bag od words model -> rows represent each line, each word is one column\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#Make sure only keep the top 1500 most frequent words for analysis\n",
        "cv=CountVectorizer(max_features=1500)\n",
        "#Create X and y -> X should be a large array where (# of records,#of words),y should be the \"name\" in GOT1\n",
        "X=cv.fit_transform(NewSentence).toarray()\n",
        "y=GOT1_ML.iloc[:,6].values\n",
        "#Make training and test sets for Naive Bays\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6zkdNx1yQag",
        "colab_type": "text"
      },
      "source": [
        "Model One - Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5T14Ub3X5ZP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "18400d68-372d-4738-8fa9-825aa082c410"
      },
      "source": [
        "#Fitting Naive Bays into the training set\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "classifier1=GaussianNB()\n",
        "classifier1.fit(X_train,y_train)\n",
        "#Predicting the test set\n",
        "y_pred_nb = classifier1.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test,y_pred_nb)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4090909090909091"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmaC6depydD8",
        "colab_type": "text"
      },
      "source": [
        "Model Two - Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJISVqoOyaQR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d757539c-db1b-4059-c620-46f93a15fd3d"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "classifier2=DecisionTreeClassifier(criterion='entropy',random_state=0)\n",
        "classifier2.fit(X_train,y_train)\n",
        "y_pred_tree = classifier2.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test,y_pred_tree)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4572192513368984"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5obIq_7cyszZ",
        "colab_type": "text"
      },
      "source": [
        "Model Three - Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xdSH3jtyrtE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c6d8872e-8344-4f2f-829d-a0de7ab67a62"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier3=RandomForestClassifier(n_estimators=10,criterion='entropy',random_state=0)\n",
        "classifier3.fit(X_train,y_train)\n",
        "y_pred_random = classifier3.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test,y_pred_random)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.48663101604278075"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDpJDDTuy3Ay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}